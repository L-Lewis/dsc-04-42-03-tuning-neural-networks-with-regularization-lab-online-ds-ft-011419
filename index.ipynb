{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from keras import models, layers, optimizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29561</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>I want to file a \" Bait and Switch '' Complain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26640</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>I am an account holder for my personal, busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>they took my whole social security check i had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24594</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>This is in dispute of my Case number : XXXX. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24249</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>My Bluebird card that i used for bill pay was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product  \\\n",
       "29561            Consumer Loan   \n",
       "26640  Bank account or service   \n",
       "24498  Bank account or service   \n",
       "24594  Bank account or service   \n",
       "24249  Bank account or service   \n",
       "\n",
       "                            Consumer complaint narrative  \n",
       "29561  I want to file a \" Bait and Switch '' Complain...  \n",
       "26640  I am an account holder for my personal, busine...  \n",
       "24498  they took my whole social security check i had...  \n",
       "24594  This is in dispute of my Case number : XXXX. I...  \n",
       "24249  My Bluebird card that i used for bill pay was ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "np.random.seed(123)\n",
    "df = df.sample(10000)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "complaints = df['Consumer complaint narrative']\n",
    "tokenizer = Tokenizer(num_words = 2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the product labels to numerical values\n",
    "product = df[\"Product\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "# Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.9562 - acc: 0.1553 - val_loss: 1.9384 - val_acc: 0.1570\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9297 - acc: 0.1865 - val_loss: 1.9206 - val_acc: 0.1890\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9097 - acc: 0.2132 - val_loss: 1.9038 - val_acc: 0.2080\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8900 - acc: 0.2381 - val_loss: 1.8850 - val_acc: 0.2370\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8680 - acc: 0.2704 - val_loss: 1.8628 - val_acc: 0.2700\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8425 - acc: 0.2960 - val_loss: 1.8366 - val_acc: 0.3010\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8119 - acc: 0.3235 - val_loss: 1.8046 - val_acc: 0.3250\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7762 - acc: 0.3447 - val_loss: 1.7674 - val_acc: 0.3500\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7353 - acc: 0.3651 - val_loss: 1.7257 - val_acc: 0.3770\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6903 - acc: 0.3900 - val_loss: 1.6800 - val_acc: 0.3970\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6419 - acc: 0.4091 - val_loss: 1.6315 - val_acc: 0.4190\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5912 - acc: 0.4299 - val_loss: 1.5817 - val_acc: 0.4450\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5391 - acc: 0.4592 - val_loss: 1.5302 - val_acc: 0.4690\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4863 - acc: 0.4856 - val_loss: 1.4792 - val_acc: 0.4880\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4339 - acc: 0.5101 - val_loss: 1.4280 - val_acc: 0.5070\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3826 - acc: 0.5359 - val_loss: 1.3791 - val_acc: 0.5350\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3333 - acc: 0.5561 - val_loss: 1.3317 - val_acc: 0.5510\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2858 - acc: 0.5759 - val_loss: 1.2866 - val_acc: 0.5720\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2404 - acc: 0.6031 - val_loss: 1.2440 - val_acc: 0.5820\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1971 - acc: 0.6147 - val_loss: 1.2027 - val_acc: 0.6020\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1562 - acc: 0.6332 - val_loss: 1.1660 - val_acc: 0.6150\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1181 - acc: 0.6444 - val_loss: 1.1302 - val_acc: 0.6170\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0822 - acc: 0.6529 - val_loss: 1.0966 - val_acc: 0.6390\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0486 - acc: 0.6635 - val_loss: 1.0651 - val_acc: 0.6500\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0165 - acc: 0.6739 - val_loss: 1.0380 - val_acc: 0.6560\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9874 - acc: 0.6807 - val_loss: 1.0110 - val_acc: 0.6700\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9598 - acc: 0.6856 - val_loss: 0.9863 - val_acc: 0.6740\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9343 - acc: 0.6955 - val_loss: 0.9625 - val_acc: 0.6810\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9104 - acc: 0.6999 - val_loss: 0.9415 - val_acc: 0.6810\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8881 - acc: 0.7065 - val_loss: 0.9234 - val_acc: 0.6860\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8673 - acc: 0.7125 - val_loss: 0.9045 - val_acc: 0.6910\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8482 - acc: 0.7155 - val_loss: 0.8880 - val_acc: 0.6940\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8292 - acc: 0.7207 - val_loss: 0.8720 - val_acc: 0.6990\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8121 - acc: 0.7255 - val_loss: 0.8603 - val_acc: 0.7050\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7972 - acc: 0.7283 - val_loss: 0.8464 - val_acc: 0.7070\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7817 - acc: 0.7347 - val_loss: 0.8331 - val_acc: 0.7090\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7678 - acc: 0.7380 - val_loss: 0.8203 - val_acc: 0.7130\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7543 - acc: 0.7453 - val_loss: 0.8102 - val_acc: 0.7230\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7416 - acc: 0.7467 - val_loss: 0.7989 - val_acc: 0.7230\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7296 - acc: 0.7507 - val_loss: 0.7893 - val_acc: 0.7230\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7180 - acc: 0.7560 - val_loss: 0.7814 - val_acc: 0.7250\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7080 - acc: 0.7572 - val_loss: 0.7741 - val_acc: 0.7300\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6975 - acc: 0.7621 - val_loss: 0.7638 - val_acc: 0.7300\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6876 - acc: 0.7649 - val_loss: 0.7549 - val_acc: 0.7320\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6782 - acc: 0.7661 - val_loss: 0.7509 - val_acc: 0.7320\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6694 - acc: 0.7715 - val_loss: 0.7447 - val_acc: 0.7380\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6613 - acc: 0.7745 - val_loss: 0.7375 - val_acc: 0.7350\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6524 - acc: 0.7756 - val_loss: 0.7314 - val_acc: 0.7430\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6446 - acc: 0.7821 - val_loss: 0.7287 - val_acc: 0.7390\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6374 - acc: 0.7865 - val_loss: 0.7220 - val_acc: 0.7420\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6294 - acc: 0.7860 - val_loss: 0.7175 - val_acc: 0.7400\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6231 - acc: 0.7880 - val_loss: 0.7116 - val_acc: 0.7510\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6155 - acc: 0.7913 - val_loss: 0.7068 - val_acc: 0.7370\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6089 - acc: 0.7947 - val_loss: 0.7032 - val_acc: 0.7450\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6023 - acc: 0.7959 - val_loss: 0.6957 - val_acc: 0.7520\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5962 - acc: 0.7976 - val_loss: 0.6942 - val_acc: 0.7450\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5900 - acc: 0.7989 - val_loss: 0.6905 - val_acc: 0.7490\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5846 - acc: 0.8016 - val_loss: 0.6864 - val_acc: 0.7500\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5782 - acc: 0.8053 - val_loss: 0.6845 - val_acc: 0.7540\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5727 - acc: 0.8063 - val_loss: 0.6795 - val_acc: 0.7490\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5677 - acc: 0.8093 - val_loss: 0.6792 - val_acc: 0.7460\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5624 - acc: 0.8097 - val_loss: 0.6731 - val_acc: 0.7520\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5570 - acc: 0.8131 - val_loss: 0.6701 - val_acc: 0.7510\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5517 - acc: 0.8147 - val_loss: 0.6673 - val_acc: 0.7520\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5469 - acc: 0.8196 - val_loss: 0.6658 - val_acc: 0.7500\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5417 - acc: 0.8189 - val_loss: 0.6673 - val_acc: 0.7540\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5367 - acc: 0.8215 - val_loss: 0.6584 - val_acc: 0.7540\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5324 - acc: 0.8207 - val_loss: 0.6576 - val_acc: 0.7540\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5275 - acc: 0.8241 - val_loss: 0.6560 - val_acc: 0.7470\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5225 - acc: 0.8255 - val_loss: 0.6546 - val_acc: 0.7590\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5184 - acc: 0.8271 - val_loss: 0.6538 - val_acc: 0.7610\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5144 - acc: 0.8273 - val_loss: 0.6495 - val_acc: 0.7540\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5098 - acc: 0.8299 - val_loss: 0.6473 - val_acc: 0.7590\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5059 - acc: 0.8325 - val_loss: 0.6467 - val_acc: 0.7640\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5015 - acc: 0.8325 - val_loss: 0.6435 - val_acc: 0.7520\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4974 - acc: 0.8339 - val_loss: 0.6424 - val_acc: 0.7510\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4936 - acc: 0.8357 - val_loss: 0.6399 - val_acc: 0.7580\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4893 - acc: 0.8357 - val_loss: 0.6393 - val_acc: 0.7600\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4858 - acc: 0.8368 - val_loss: 0.6386 - val_acc: 0.7560\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4815 - acc: 0.8383 - val_loss: 0.6367 - val_acc: 0.7620\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4780 - acc: 0.8409 - val_loss: 0.6389 - val_acc: 0.7580\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4738 - acc: 0.8441 - val_loss: 0.6335 - val_acc: 0.7610\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4701 - acc: 0.8451 - val_loss: 0.6372 - val_acc: 0.7640\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4668 - acc: 0.8437 - val_loss: 0.6298 - val_acc: 0.7600\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4634 - acc: 0.8469 - val_loss: 0.6322 - val_acc: 0.7660\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4595 - acc: 0.8475 - val_loss: 0.6296 - val_acc: 0.7600\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.4559 - acc: 0.8489 - val_loss: 0.6277 - val_acc: 0.7620\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4525 - acc: 0.8505 - val_loss: 0.6300 - val_acc: 0.7610\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4487 - acc: 0.8533 - val_loss: 0.6280 - val_acc: 0.7590\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4453 - acc: 0.8529 - val_loss: 0.6261 - val_acc: 0.7650\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4417 - acc: 0.8561 - val_loss: 0.6245 - val_acc: 0.7640\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4387 - acc: 0.8571 - val_loss: 0.6244 - val_acc: 0.7660\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4350 - acc: 0.8573 - val_loss: 0.6240 - val_acc: 0.7660\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4319 - acc: 0.8580 - val_loss: 0.6276 - val_acc: 0.7620\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4287 - acc: 0.8605 - val_loss: 0.6223 - val_acc: 0.7630\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4256 - acc: 0.8625 - val_loss: 0.6203 - val_acc: 0.7590\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.4226 - acc: 0.8633 - val_loss: 0.6202 - val_acc: 0.7630\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4189 - acc: 0.8644 - val_loss: 0.6282 - val_acc: 0.7620\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.4160 - acc: 0.8657 - val_loss: 0.6204 - val_acc: 0.7610\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4130 - acc: 0.8664 - val_loss: 0.6223 - val_acc: 0.7610\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4097 - acc: 0.8687 - val_loss: 0.6231 - val_acc: 0.7660\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4068 - acc: 0.8675 - val_loss: 0.6222 - val_acc: 0.7660\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4038 - acc: 0.8693 - val_loss: 0.6210 - val_acc: 0.7670\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4012 - acc: 0.8716 - val_loss: 0.6203 - val_acc: 0.7590\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.3982 - acc: 0.8724 - val_loss: 0.6173 - val_acc: 0.7610\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3950 - acc: 0.8739 - val_loss: 0.6249 - val_acc: 0.7590\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3923 - acc: 0.8733 - val_loss: 0.6193 - val_acc: 0.7660\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3890 - acc: 0.8747 - val_loss: 0.6213 - val_acc: 0.7610\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3866 - acc: 0.8760 - val_loss: 0.6199 - val_acc: 0.7620\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3834 - acc: 0.8783 - val_loss: 0.6202 - val_acc: 0.7590\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3812 - acc: 0.8784 - val_loss: 0.6172 - val_acc: 0.7640\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3780 - acc: 0.8799 - val_loss: 0.6169 - val_acc: 0.7650\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3752 - acc: 0.8803 - val_loss: 0.6199 - val_acc: 0.7700\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3727 - acc: 0.8781 - val_loss: 0.6182 - val_acc: 0.7560\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3699 - acc: 0.8829 - val_loss: 0.6208 - val_acc: 0.7640\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.3671 - acc: 0.8813 - val_loss: 0.6179 - val_acc: 0.7630\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3642 - acc: 0.8840 - val_loss: 0.6199 - val_acc: 0.7620\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3618 - acc: 0.8831 - val_loss: 0.6171 - val_acc: 0.7650\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3595 - acc: 0.8856 - val_loss: 0.6178 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3568 - acc: 0.8863 - val_loss: 0.6190 - val_acc: 0.7660\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data = (val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3531909782489141, 0.8869333333333334]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6362837637265524, 0.761333333492279]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9578 - acc: 0.1635 - val_loss: 1.9490 - val_acc: 0.1990\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9419 - acc: 0.1841 - val_loss: 1.9363 - val_acc: 0.2130\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9285 - acc: 0.2039 - val_loss: 1.9246 - val_acc: 0.2170\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9148 - acc: 0.2221 - val_loss: 1.9129 - val_acc: 0.2220\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8999 - acc: 0.2396 - val_loss: 1.8995 - val_acc: 0.2420\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8831 - acc: 0.2549 - val_loss: 1.8830 - val_acc: 0.2570\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8637 - acc: 0.2695 - val_loss: 1.8650 - val_acc: 0.2740\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8417 - acc: 0.2937 - val_loss: 1.8431 - val_acc: 0.2850\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8163 - acc: 0.3169 - val_loss: 1.8181 - val_acc: 0.3140\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7875 - acc: 0.3397 - val_loss: 1.7883 - val_acc: 0.3350\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7552 - acc: 0.3751 - val_loss: 1.7560 - val_acc: 0.3650\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7195 - acc: 0.4025 - val_loss: 1.7189 - val_acc: 0.4090\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6803 - acc: 0.4333 - val_loss: 1.6788 - val_acc: 0.4440\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6380 - acc: 0.4651 - val_loss: 1.6376 - val_acc: 0.4650\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5924 - acc: 0.4905 - val_loss: 1.5913 - val_acc: 0.5050\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5438 - acc: 0.5183 - val_loss: 1.5419 - val_acc: 0.5360\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4923 - acc: 0.5473 - val_loss: 1.4914 - val_acc: 0.5660\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4393 - acc: 0.5749 - val_loss: 1.4400 - val_acc: 0.5750\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3847 - acc: 0.5980 - val_loss: 1.3850 - val_acc: 0.6050\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3306 - acc: 0.6187 - val_loss: 1.3328 - val_acc: 0.6160\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2767 - acc: 0.6355 - val_loss: 1.2815 - val_acc: 0.6270\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2246 - acc: 0.6512 - val_loss: 1.2318 - val_acc: 0.6480\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1746 - acc: 0.6628 - val_loss: 1.1862 - val_acc: 0.6470\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1273 - acc: 0.6747 - val_loss: 1.1378 - val_acc: 0.6630\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0825 - acc: 0.6863 - val_loss: 1.0978 - val_acc: 0.6670\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0412 - acc: 0.6939 - val_loss: 1.0584 - val_acc: 0.6760\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0031 - acc: 0.7028 - val_loss: 1.0245 - val_acc: 0.6900\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9683 - acc: 0.7117 - val_loss: 0.9923 - val_acc: 0.6920\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9364 - acc: 0.7148 - val_loss: 0.9613 - val_acc: 0.7030\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9076 - acc: 0.7201 - val_loss: 0.9381 - val_acc: 0.7070\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8812 - acc: 0.7247 - val_loss: 0.9112 - val_acc: 0.7110\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8567 - acc: 0.7248 - val_loss: 0.8904 - val_acc: 0.7110\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8346 - acc: 0.7303 - val_loss: 0.8737 - val_acc: 0.7170\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8144 - acc: 0.7344 - val_loss: 0.8564 - val_acc: 0.7220\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7960 - acc: 0.7375 - val_loss: 0.8383 - val_acc: 0.7220\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7788 - acc: 0.7397 - val_loss: 0.8242 - val_acc: 0.7290\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7636 - acc: 0.7427 - val_loss: 0.8088 - val_acc: 0.7280\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7487 - acc: 0.7465 - val_loss: 0.7999 - val_acc: 0.7300\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7353 - acc: 0.7505 - val_loss: 0.7911 - val_acc: 0.7360\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7228 - acc: 0.7540 - val_loss: 0.7764 - val_acc: 0.7430\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7113 - acc: 0.7561 - val_loss: 0.7659 - val_acc: 0.7400\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7002 - acc: 0.7596 - val_loss: 0.7590 - val_acc: 0.7400\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6897 - acc: 0.7623 - val_loss: 0.7552 - val_acc: 0.7420\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6800 - acc: 0.7633 - val_loss: 0.7466 - val_acc: 0.7400\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6709 - acc: 0.7681 - val_loss: 0.7370 - val_acc: 0.7400\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6624 - acc: 0.7680 - val_loss: 0.7339 - val_acc: 0.7390\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6538 - acc: 0.7713 - val_loss: 0.7278 - val_acc: 0.7420\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6456 - acc: 0.7733 - val_loss: 0.7233 - val_acc: 0.7480\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6385 - acc: 0.7764 - val_loss: 0.7153 - val_acc: 0.7480\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6308 - acc: 0.7772 - val_loss: 0.7101 - val_acc: 0.7430\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6235 - acc: 0.7797 - val_loss: 0.7046 - val_acc: 0.7480\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6168 - acc: 0.7836 - val_loss: 0.7006 - val_acc: 0.7480\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6101 - acc: 0.7867 - val_loss: 0.6999 - val_acc: 0.7500\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6043 - acc: 0.7869 - val_loss: 0.6947 - val_acc: 0.7510\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5980 - acc: 0.7904 - val_loss: 0.6897 - val_acc: 0.7520\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5915 - acc: 0.7909 - val_loss: 0.6871 - val_acc: 0.7550\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5860 - acc: 0.7920 - val_loss: 0.6847 - val_acc: 0.7420\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5805 - acc: 0.7952 - val_loss: 0.6805 - val_acc: 0.7530\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5747 - acc: 0.7995 - val_loss: 0.6757 - val_acc: 0.7490\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5692 - acc: 0.8000 - val_loss: 0.6747 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5650803737640381, 0.7986666666984558]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6826639523506165, 0.7346666669845581]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.6071 - acc: 0.1516 - val_loss: 2.5999 - val_acc: 0.1360\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.5849 - acc: 0.1796 - val_loss: 2.5832 - val_acc: 0.1610\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.5687 - acc: 0.1987 - val_loss: 2.5694 - val_acc: 0.1700\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.5535 - acc: 0.2164 - val_loss: 2.5559 - val_acc: 0.1820\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.5383 - acc: 0.2317 - val_loss: 2.5413 - val_acc: 0.2040\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.5218 - acc: 0.2471 - val_loss: 2.5255 - val_acc: 0.2170\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.5030 - acc: 0.2608 - val_loss: 2.5070 - val_acc: 0.2310\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.4806 - acc: 0.2804 - val_loss: 2.4855 - val_acc: 0.2450\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.4548 - acc: 0.3028 - val_loss: 2.4603 - val_acc: 0.2750\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.4253 - acc: 0.3275 - val_loss: 2.4327 - val_acc: 0.2900\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.3917 - acc: 0.3483 - val_loss: 2.3991 - val_acc: 0.3270\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.3542 - acc: 0.3836 - val_loss: 2.3624 - val_acc: 0.3710\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.3132 - acc: 0.4188 - val_loss: 2.3199 - val_acc: 0.4200\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.2691 - acc: 0.4505 - val_loss: 2.2755 - val_acc: 0.4540\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.2225 - acc: 0.4883 - val_loss: 2.2323 - val_acc: 0.4510\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.1744 - acc: 0.5164 - val_loss: 2.1848 - val_acc: 0.4860\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.1255 - acc: 0.5393 - val_loss: 2.1340 - val_acc: 0.5350\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.0764 - acc: 0.5643 - val_loss: 2.0874 - val_acc: 0.5480\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.0276 - acc: 0.5856 - val_loss: 2.0385 - val_acc: 0.5790\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9794 - acc: 0.6021 - val_loss: 1.9906 - val_acc: 0.5990\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9323 - acc: 0.6191 - val_loss: 1.9466 - val_acc: 0.6130\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8865 - acc: 0.6340 - val_loss: 1.9004 - val_acc: 0.6150\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8424 - acc: 0.6475 - val_loss: 1.8571 - val_acc: 0.6350\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8005 - acc: 0.6561 - val_loss: 1.8233 - val_acc: 0.6340\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7606 - acc: 0.6669 - val_loss: 1.7792 - val_acc: 0.6530\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7226 - acc: 0.6748 - val_loss: 1.7432 - val_acc: 0.6590\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6870 - acc: 0.6847 - val_loss: 1.7100 - val_acc: 0.6710\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6530 - acc: 0.6953 - val_loss: 1.6767 - val_acc: 0.6830\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6212 - acc: 0.7008 - val_loss: 1.6485 - val_acc: 0.6830\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5909 - acc: 0.7087 - val_loss: 1.6232 - val_acc: 0.6890\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5625 - acc: 0.7164 - val_loss: 1.5940 - val_acc: 0.6930\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5358 - acc: 0.7232 - val_loss: 1.5687 - val_acc: 0.6960\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5106 - acc: 0.7272 - val_loss: 1.5474 - val_acc: 0.7020\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4863 - acc: 0.7335 - val_loss: 1.5236 - val_acc: 0.7010\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4643 - acc: 0.7365 - val_loss: 1.5015 - val_acc: 0.7120\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4427 - acc: 0.7400 - val_loss: 1.4851 - val_acc: 0.7090\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4225 - acc: 0.7460 - val_loss: 1.4692 - val_acc: 0.7160\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4038 - acc: 0.7487 - val_loss: 1.4495 - val_acc: 0.7150\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3857 - acc: 0.7533 - val_loss: 1.4369 - val_acc: 0.7220\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3690 - acc: 0.7561 - val_loss: 1.4192 - val_acc: 0.7310\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3523 - acc: 0.7577 - val_loss: 1.4077 - val_acc: 0.7390\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3372 - acc: 0.7627 - val_loss: 1.3912 - val_acc: 0.7330\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3224 - acc: 0.7649 - val_loss: 1.3806 - val_acc: 0.7340\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3087 - acc: 0.7676 - val_loss: 1.3684 - val_acc: 0.7450\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2956 - acc: 0.7725 - val_loss: 1.3590 - val_acc: 0.7320\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2833 - acc: 0.7748 - val_loss: 1.3464 - val_acc: 0.7370\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2704 - acc: 0.7775 - val_loss: 1.3390 - val_acc: 0.7320\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2594 - acc: 0.7799 - val_loss: 1.3312 - val_acc: 0.7500\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2481 - acc: 0.7819 - val_loss: 1.3187 - val_acc: 0.7530\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2375 - acc: 0.7849 - val_loss: 1.3092 - val_acc: 0.7540\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2273 - acc: 0.7876 - val_loss: 1.3021 - val_acc: 0.7470\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2175 - acc: 0.7901 - val_loss: 1.2962 - val_acc: 0.7450\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2078 - acc: 0.7909 - val_loss: 1.2923 - val_acc: 0.7400\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1989 - acc: 0.7939 - val_loss: 1.2816 - val_acc: 0.7430\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1892 - acc: 0.7984 - val_loss: 1.2785 - val_acc: 0.7510\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1809 - acc: 0.7981 - val_loss: 1.2667 - val_acc: 0.7550\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1728 - acc: 0.7979 - val_loss: 1.2595 - val_acc: 0.7540\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1644 - acc: 0.8009 - val_loss: 1.2522 - val_acc: 0.7590\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1564 - acc: 0.8032 - val_loss: 1.2495 - val_acc: 0.7490\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1489 - acc: 0.8047 - val_loss: 1.2425 - val_acc: 0.7570\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1413 - acc: 0.8063 - val_loss: 1.2362 - val_acc: 0.7590\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1340 - acc: 0.8088 - val_loss: 1.2345 - val_acc: 0.7510\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1266 - acc: 0.8121 - val_loss: 1.2274 - val_acc: 0.7590\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1192 - acc: 0.8123 - val_loss: 1.2233 - val_acc: 0.7550\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1127 - acc: 0.8132 - val_loss: 1.2155 - val_acc: 0.7580\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1058 - acc: 0.8152 - val_loss: 1.2149 - val_acc: 0.7580\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0988 - acc: 0.8169 - val_loss: 1.2062 - val_acc: 0.7600\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0923 - acc: 0.8192 - val_loss: 1.2055 - val_acc: 0.7570\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0860 - acc: 0.8212 - val_loss: 1.1974 - val_acc: 0.7580\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0796 - acc: 0.8221 - val_loss: 1.1949 - val_acc: 0.7600\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0736 - acc: 0.8247 - val_loss: 1.1934 - val_acc: 0.7560\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0675 - acc: 0.8252 - val_loss: 1.1899 - val_acc: 0.7550\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0613 - acc: 0.8269 - val_loss: 1.1833 - val_acc: 0.7590\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0557 - acc: 0.8277 - val_loss: 1.1774 - val_acc: 0.7560\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0496 - acc: 0.8320 - val_loss: 1.1754 - val_acc: 0.7600\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0445 - acc: 0.8328 - val_loss: 1.1699 - val_acc: 0.7580\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0385 - acc: 0.8333 - val_loss: 1.1684 - val_acc: 0.7590\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0328 - acc: 0.8349 - val_loss: 1.1632 - val_acc: 0.7580\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0269 - acc: 0.8381 - val_loss: 1.1627 - val_acc: 0.7510\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0220 - acc: 0.8397 - val_loss: 1.1577 - val_acc: 0.7620\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0166 - acc: 0.8423 - val_loss: 1.1552 - val_acc: 0.7630\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0114 - acc: 0.8417 - val_loss: 1.1543 - val_acc: 0.7500\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0061 - acc: 0.8416 - val_loss: 1.1494 - val_acc: 0.7580\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0008 - acc: 0.8452 - val_loss: 1.1475 - val_acc: 0.7560\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9962 - acc: 0.8459 - val_loss: 1.1401 - val_acc: 0.7640\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9910 - acc: 0.8484 - val_loss: 1.1386 - val_acc: 0.7630\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9858 - acc: 0.8465 - val_loss: 1.1454 - val_acc: 0.7490\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9811 - acc: 0.8499 - val_loss: 1.1325 - val_acc: 0.7670\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9765 - acc: 0.8496 - val_loss: 1.1348 - val_acc: 0.7550\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9716 - acc: 0.8521 - val_loss: 1.1293 - val_acc: 0.7540\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9664 - acc: 0.8531 - val_loss: 1.1263 - val_acc: 0.7520\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9619 - acc: 0.8540 - val_loss: 1.1215 - val_acc: 0.7630\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9577 - acc: 0.8560 - val_loss: 1.1228 - val_acc: 0.7620\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9531 - acc: 0.8557 - val_loss: 1.1204 - val_acc: 0.7640\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9481 - acc: 0.8567 - val_loss: 1.1156 - val_acc: 0.7620\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9439 - acc: 0.8581 - val_loss: 1.1132 - val_acc: 0.7640\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9395 - acc: 0.8599 - val_loss: 1.1109 - val_acc: 0.7620\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9351 - acc: 0.8604 - val_loss: 1.1092 - val_acc: 0.7610\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9306 - acc: 0.8625 - val_loss: 1.1059 - val_acc: 0.7550\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9266 - acc: 0.8608 - val_loss: 1.1040 - val_acc: 0.7610\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9223 - acc: 0.8667 - val_loss: 1.1003 - val_acc: 0.7610\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9178 - acc: 0.8661 - val_loss: 1.0971 - val_acc: 0.7640\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9137 - acc: 0.8645 - val_loss: 1.0958 - val_acc: 0.7630\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9092 - acc: 0.8664 - val_loss: 1.0956 - val_acc: 0.7680\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9053 - acc: 0.8659 - val_loss: 1.0912 - val_acc: 0.7600\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9015 - acc: 0.8683 - val_loss: 1.0916 - val_acc: 0.7610\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8979 - acc: 0.8703 - val_loss: 1.0887 - val_acc: 0.7610\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8937 - acc: 0.8692 - val_loss: 1.0869 - val_acc: 0.7650\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8896 - acc: 0.8719 - val_loss: 1.0835 - val_acc: 0.7680\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8860 - acc: 0.8724 - val_loss: 1.0809 - val_acc: 0.7660\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8816 - acc: 0.8724 - val_loss: 1.0770 - val_acc: 0.7680\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8779 - acc: 0.8743 - val_loss: 1.0767 - val_acc: 0.7630\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8739 - acc: 0.8756 - val_loss: 1.0750 - val_acc: 0.7630\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8702 - acc: 0.8759 - val_loss: 1.0748 - val_acc: 0.7640\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8672 - acc: 0.8757 - val_loss: 1.0721 - val_acc: 0.7630\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8632 - acc: 0.8772 - val_loss: 1.0687 - val_acc: 0.7660\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8597 - acc: 0.8784 - val_loss: 1.0703 - val_acc: 0.7640\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8561 - acc: 0.8789 - val_loss: 1.0677 - val_acc: 0.7650\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8521 - acc: 0.8795 - val_loss: 1.0640 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8487 - acc: 0.8788 - val_loss: 1.0615 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'b', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'orange', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 15.9949 - acc: 0.1757 - val_loss: 15.5930 - val_acc: 0.1830\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 15.2386 - acc: 0.1924 - val_loss: 14.8539 - val_acc: 0.1850\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 14.5067 - acc: 0.2011 - val_loss: 14.1378 - val_acc: 0.1990\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 13.7974 - acc: 0.2152 - val_loss: 13.4422 - val_acc: 0.2060\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 13.1086 - acc: 0.2327 - val_loss: 12.7658 - val_acc: 0.2170\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 12.4374 - acc: 0.2512 - val_loss: 12.1045 - val_acc: 0.2410\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 11.7819 - acc: 0.2753 - val_loss: 11.4583 - val_acc: 0.2580\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 11.1436 - acc: 0.3055 - val_loss: 10.8304 - val_acc: 0.2790\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 10.5246 - acc: 0.3283 - val_loss: 10.2225 - val_acc: 0.3400\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 9.9254 - acc: 0.3688 - val_loss: 9.6351 - val_acc: 0.3730\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 9.3465 - acc: 0.3999 - val_loss: 9.0671 - val_acc: 0.4190\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 8.7877 - acc: 0.4339 - val_loss: 8.5211 - val_acc: 0.4350\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 8.2504 - acc: 0.4555 - val_loss: 7.9960 - val_acc: 0.4510\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.7351 - acc: 0.4704 - val_loss: 7.4917 - val_acc: 0.4850\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 7.2416 - acc: 0.4987 - val_loss: 7.0109 - val_acc: 0.4920\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 6.7712 - acc: 0.5161 - val_loss: 6.5526 - val_acc: 0.5050\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 6.3240 - acc: 0.5345 - val_loss: 6.1179 - val_acc: 0.5350\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 5.9003 - acc: 0.5584 - val_loss: 5.7068 - val_acc: 0.5520\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 5.4997 - acc: 0.5787 - val_loss: 5.3182 - val_acc: 0.5590\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 5.1226 - acc: 0.5927 - val_loss: 4.9547 - val_acc: 0.5710\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 4.7691 - acc: 0.6051 - val_loss: 4.6116 - val_acc: 0.5930\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 4.4389 - acc: 0.6201 - val_loss: 4.2935 - val_acc: 0.6010\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 4.1319 - acc: 0.6307 - val_loss: 3.9998 - val_acc: 0.6110\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.8476 - acc: 0.6393 - val_loss: 3.7246 - val_acc: 0.6250\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 3.5854 - acc: 0.6453 - val_loss: 3.4739 - val_acc: 0.6520\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.3458 - acc: 0.6584 - val_loss: 3.2443 - val_acc: 0.6590\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.1285 - acc: 0.6639 - val_loss: 3.0378 - val_acc: 0.6590\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.9327 - acc: 0.6687 - val_loss: 2.8588 - val_acc: 0.6480\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.7592 - acc: 0.6721 - val_loss: 2.6898 - val_acc: 0.6660\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.6063 - acc: 0.6769 - val_loss: 2.5501 - val_acc: 0.6560\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.4737 - acc: 0.6767 - val_loss: 2.4259 - val_acc: 0.6670\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.3604 - acc: 0.6797 - val_loss: 2.3233 - val_acc: 0.6630\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.2662 - acc: 0.6792 - val_loss: 2.2371 - val_acc: 0.6590\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.1902 - acc: 0.6799 - val_loss: 2.1688 - val_acc: 0.6680\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.1312 - acc: 0.6823 - val_loss: 2.1168 - val_acc: 0.6740\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.0873 - acc: 0.6849 - val_loss: 2.0792 - val_acc: 0.6740\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.0544 - acc: 0.6847 - val_loss: 2.0495 - val_acc: 0.6760\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.0280 - acc: 0.6845 - val_loss: 2.0270 - val_acc: 0.6790\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.0044 - acc: 0.6869 - val_loss: 2.0055 - val_acc: 0.6760\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9837 - acc: 0.6899 - val_loss: 1.9815 - val_acc: 0.6830\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9639 - acc: 0.6904 - val_loss: 1.9646 - val_acc: 0.6780\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9452 - acc: 0.6916 - val_loss: 1.9455 - val_acc: 0.6840\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9279 - acc: 0.6903 - val_loss: 1.9266 - val_acc: 0.6900\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9110 - acc: 0.6913 - val_loss: 1.9097 - val_acc: 0.6900\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8950 - acc: 0.6916 - val_loss: 1.8934 - val_acc: 0.6900\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8798 - acc: 0.6921 - val_loss: 1.8814 - val_acc: 0.6890\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8651 - acc: 0.6935 - val_loss: 1.8642 - val_acc: 0.6870\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8504 - acc: 0.6945 - val_loss: 1.8507 - val_acc: 0.6860\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8366 - acc: 0.6943 - val_loss: 1.8382 - val_acc: 0.6940\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8231 - acc: 0.6941 - val_loss: 1.8237 - val_acc: 0.6940\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8102 - acc: 0.6947 - val_loss: 1.8146 - val_acc: 0.7000\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7979 - acc: 0.6957 - val_loss: 1.8005 - val_acc: 0.6960\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7858 - acc: 0.6960 - val_loss: 1.7865 - val_acc: 0.6980\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7737 - acc: 0.6957 - val_loss: 1.7736 - val_acc: 0.6970\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7621 - acc: 0.6948 - val_loss: 1.7612 - val_acc: 0.7000\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7510 - acc: 0.6971 - val_loss: 1.7539 - val_acc: 0.6960\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7399 - acc: 0.6985 - val_loss: 1.7401 - val_acc: 0.6970\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7294 - acc: 0.6993 - val_loss: 1.7295 - val_acc: 0.7060\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7190 - acc: 0.6981 - val_loss: 1.7186 - val_acc: 0.7030\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7089 - acc: 0.6992 - val_loss: 1.7099 - val_acc: 0.7020\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6993 - acc: 0.6992 - val_loss: 1.7015 - val_acc: 0.7090\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6895 - acc: 0.6979 - val_loss: 1.6910 - val_acc: 0.7100\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6792 - acc: 0.7016 - val_loss: 1.6850 - val_acc: 0.6990\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6699 - acc: 0.7012 - val_loss: 1.6702 - val_acc: 0.7090\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6610 - acc: 0.7001 - val_loss: 1.6605 - val_acc: 0.7130\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6515 - acc: 0.7019 - val_loss: 1.6517 - val_acc: 0.7120\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6430 - acc: 0.7035 - val_loss: 1.6447 - val_acc: 0.7070\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6347 - acc: 0.7029 - val_loss: 1.6355 - val_acc: 0.7120\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6261 - acc: 0.7039 - val_loss: 1.6258 - val_acc: 0.7110\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6174 - acc: 0.7048 - val_loss: 1.6200 - val_acc: 0.7060\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6095 - acc: 0.7035 - val_loss: 1.6101 - val_acc: 0.7130\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6020 - acc: 0.7049 - val_loss: 1.6060 - val_acc: 0.7130\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5934 - acc: 0.7056 - val_loss: 1.5933 - val_acc: 0.7130\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5864 - acc: 0.7064 - val_loss: 1.5862 - val_acc: 0.7150\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5780 - acc: 0.7052 - val_loss: 1.5770 - val_acc: 0.7130\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5704 - acc: 0.7072 - val_loss: 1.5697 - val_acc: 0.7180\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5634 - acc: 0.7064 - val_loss: 1.5626 - val_acc: 0.7120\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5558 - acc: 0.7069 - val_loss: 1.5608 - val_acc: 0.7130\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5489 - acc: 0.7088 - val_loss: 1.5490 - val_acc: 0.7190\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5411 - acc: 0.7101 - val_loss: 1.5394 - val_acc: 0.7160\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5340 - acc: 0.7096 - val_loss: 1.5347 - val_acc: 0.7160\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5272 - acc: 0.7104 - val_loss: 1.5261 - val_acc: 0.7160\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5201 - acc: 0.7105 - val_loss: 1.5205 - val_acc: 0.7150\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5136 - acc: 0.7105 - val_loss: 1.5138 - val_acc: 0.7180\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5066 - acc: 0.7101 - val_loss: 1.5063 - val_acc: 0.7150\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5000 - acc: 0.7125 - val_loss: 1.5060 - val_acc: 0.7110\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4932 - acc: 0.7117 - val_loss: 1.4934 - val_acc: 0.7220\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4869 - acc: 0.7123 - val_loss: 1.4876 - val_acc: 0.7190\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4799 - acc: 0.7125 - val_loss: 1.4781 - val_acc: 0.7180\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4741 - acc: 0.7125 - val_loss: 1.4697 - val_acc: 0.7160\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4670 - acc: 0.7123 - val_loss: 1.4658 - val_acc: 0.7210\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4609 - acc: 0.7133 - val_loss: 1.4591 - val_acc: 0.7190\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4546 - acc: 0.7151 - val_loss: 1.4523 - val_acc: 0.7160\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4480 - acc: 0.7161 - val_loss: 1.4454 - val_acc: 0.7180\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4426 - acc: 0.7152 - val_loss: 1.4421 - val_acc: 0.7190\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4363 - acc: 0.7160 - val_loss: 1.4386 - val_acc: 0.7150\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4306 - acc: 0.7157 - val_loss: 1.4303 - val_acc: 0.7230\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4246 - acc: 0.7151 - val_loss: 1.4258 - val_acc: 0.7210\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4189 - acc: 0.7169 - val_loss: 1.4164 - val_acc: 0.7230\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4129 - acc: 0.7189 - val_loss: 1.4116 - val_acc: 0.7200\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4074 - acc: 0.7176 - val_loss: 1.4060 - val_acc: 0.7240\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4010 - acc: 0.7180 - val_loss: 1.3989 - val_acc: 0.7270\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3949 - acc: 0.7197 - val_loss: 1.3957 - val_acc: 0.7250\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3901 - acc: 0.7209 - val_loss: 1.3932 - val_acc: 0.7190\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3849 - acc: 0.7195 - val_loss: 1.3820 - val_acc: 0.7210\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3792 - acc: 0.7197 - val_loss: 1.3790 - val_acc: 0.7220\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3740 - acc: 0.7208 - val_loss: 1.3730 - val_acc: 0.7240\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3688 - acc: 0.7200 - val_loss: 1.3681 - val_acc: 0.7290\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3630 - acc: 0.7200 - val_loss: 1.3659 - val_acc: 0.7280\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3579 - acc: 0.7213 - val_loss: 1.3602 - val_acc: 0.7260\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3529 - acc: 0.7227 - val_loss: 1.3522 - val_acc: 0.7220\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3481 - acc: 0.7213 - val_loss: 1.3469 - val_acc: 0.7240\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3435 - acc: 0.7229 - val_loss: 1.3473 - val_acc: 0.7240\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3383 - acc: 0.7240 - val_loss: 1.3356 - val_acc: 0.7250\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3331 - acc: 0.7239 - val_loss: 1.3319 - val_acc: 0.7300\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3281 - acc: 0.7245 - val_loss: 1.3310 - val_acc: 0.7260\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3236 - acc: 0.7235 - val_loss: 1.3205 - val_acc: 0.7280\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3184 - acc: 0.7241 - val_loss: 1.3160 - val_acc: 0.7250\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3145 - acc: 0.7256 - val_loss: 1.3109 - val_acc: 0.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3096 - acc: 0.7257 - val_loss: 1.3125 - val_acc: 0.7260\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 16.0140 - acc: 0.1877 - val_loss: 15.6118 - val_acc: 0.2100\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 15.2533 - acc: 0.2067 - val_loss: 14.8668 - val_acc: 0.2240\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 14.5171 - acc: 0.2219 - val_loss: 14.1433 - val_acc: 0.2290\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.8011 - acc: 0.2360 - val_loss: 13.4388 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.1038 - acc: 0.2528 - val_loss: 12.7534 - val_acc: 0.2580\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 12.4253 - acc: 0.2772 - val_loss: 12.0869 - val_acc: 0.2720\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 11.7658 - acc: 0.3027 - val_loss: 11.4387 - val_acc: 0.3140\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.1259 - acc: 0.3431 - val_loss: 10.8103 - val_acc: 0.3470\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 10.5062 - acc: 0.3781 - val_loss: 10.2028 - val_acc: 0.3780\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.9068 - acc: 0.4124 - val_loss: 9.6137 - val_acc: 0.3970\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.3279 - acc: 0.4441 - val_loss: 9.0463 - val_acc: 0.4310\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.7708 - acc: 0.4668 - val_loss: 8.5007 - val_acc: 0.4610\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.2357 - acc: 0.4971 - val_loss: 7.9774 - val_acc: 0.4730\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.7237 - acc: 0.5213 - val_loss: 7.4776 - val_acc: 0.4990\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 7.2345 - acc: 0.5405 - val_loss: 7.0011 - val_acc: 0.5270\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.7686 - acc: 0.5620 - val_loss: 6.5474 - val_acc: 0.5490\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.3256 - acc: 0.5787 - val_loss: 6.1182 - val_acc: 0.5860\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.9053 - acc: 0.5941 - val_loss: 5.7067 - val_acc: 0.5940\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.5074 - acc: 0.6119 - val_loss: 5.3218 - val_acc: 0.5940\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.1324 - acc: 0.6183 - val_loss: 4.9565 - val_acc: 0.6200\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.7803 - acc: 0.6311 - val_loss: 4.6160 - val_acc: 0.6160\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.4521 - acc: 0.6364 - val_loss: 4.2978 - val_acc: 0.6320\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.1454 - acc: 0.6468 - val_loss: 4.0029 - val_acc: 0.6420\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8615 - acc: 0.6531 - val_loss: 3.7284 - val_acc: 0.6590\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.5996 - acc: 0.6593 - val_loss: 3.4779 - val_acc: 0.6590\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.3602 - acc: 0.6624 - val_loss: 3.2500 - val_acc: 0.6740\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.1425 - acc: 0.6635 - val_loss: 3.0417 - val_acc: 0.6780\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.9463 - acc: 0.6691 - val_loss: 2.8538 - val_acc: 0.6790\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.7710 - acc: 0.6711 - val_loss: 2.6896 - val_acc: 0.6810\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.6167 - acc: 0.6717 - val_loss: 2.5434 - val_acc: 0.6810\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4825 - acc: 0.6703 - val_loss: 2.4194 - val_acc: 0.6810\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.3687 - acc: 0.6731 - val_loss: 2.3143 - val_acc: 0.6780\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2740 - acc: 0.6723 - val_loss: 2.2270 - val_acc: 0.6820\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1980 - acc: 0.6713 - val_loss: 2.1602 - val_acc: 0.6810\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1386 - acc: 0.6725 - val_loss: 2.1087 - val_acc: 0.6800\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0942 - acc: 0.6736 - val_loss: 2.0704 - val_acc: 0.6760\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0616 - acc: 0.6712 - val_loss: 2.0393 - val_acc: 0.6720\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0343 - acc: 0.6735 - val_loss: 2.0143 - val_acc: 0.6710\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0114 - acc: 0.6720 - val_loss: 1.9902 - val_acc: 0.6790\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9903 - acc: 0.6745 - val_loss: 1.9708 - val_acc: 0.6780\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9712 - acc: 0.6747 - val_loss: 1.9519 - val_acc: 0.6830\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9536 - acc: 0.6751 - val_loss: 1.9332 - val_acc: 0.6780\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9369 - acc: 0.6753 - val_loss: 1.9168 - val_acc: 0.6820\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9213 - acc: 0.6769 - val_loss: 1.9012 - val_acc: 0.6770\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9061 - acc: 0.6791 - val_loss: 1.8871 - val_acc: 0.6790\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8920 - acc: 0.6776 - val_loss: 1.8731 - val_acc: 0.6830\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8783 - acc: 0.6800 - val_loss: 1.8585 - val_acc: 0.6820\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8650 - acc: 0.6796 - val_loss: 1.8430 - val_acc: 0.6850\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8519 - acc: 0.6815 - val_loss: 1.8301 - val_acc: 0.6900\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8395 - acc: 0.6824 - val_loss: 1.8186 - val_acc: 0.6910\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8277 - acc: 0.6844 - val_loss: 1.8058 - val_acc: 0.6900\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8157 - acc: 0.6853 - val_loss: 1.7949 - val_acc: 0.6930\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8043 - acc: 0.6844 - val_loss: 1.7821 - val_acc: 0.6950\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7931 - acc: 0.6857 - val_loss: 1.7722 - val_acc: 0.6970\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7828 - acc: 0.6863 - val_loss: 1.7612 - val_acc: 0.6880\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7722 - acc: 0.6876 - val_loss: 1.7494 - val_acc: 0.6940\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7616 - acc: 0.6877 - val_loss: 1.7391 - val_acc: 0.6920\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7517 - acc: 0.6887 - val_loss: 1.7376 - val_acc: 0.6940\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7419 - acc: 0.6892 - val_loss: 1.7217 - val_acc: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7319 - acc: 0.6887 - val_loss: 1.7170 - val_acc: 0.6940\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7227 - acc: 0.6892 - val_loss: 1.7054 - val_acc: 0.6970\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7136 - acc: 0.6903 - val_loss: 1.6913 - val_acc: 0.6980\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7041 - acc: 0.6924 - val_loss: 1.6814 - val_acc: 0.6940\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6951 - acc: 0.6928 - val_loss: 1.6735 - val_acc: 0.7040\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6864 - acc: 0.6944 - val_loss: 1.6635 - val_acc: 0.7020\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6778 - acc: 0.6957 - val_loss: 1.6551 - val_acc: 0.7050\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6691 - acc: 0.6960 - val_loss: 1.6475 - val_acc: 0.7070\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6604 - acc: 0.6987 - val_loss: 1.6402 - val_acc: 0.7010\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6518 - acc: 0.6996 - val_loss: 1.6361 - val_acc: 0.7010\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6436 - acc: 0.6987 - val_loss: 1.6220 - val_acc: 0.7000\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6354 - acc: 0.7015 - val_loss: 1.6156 - val_acc: 0.7080\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6274 - acc: 0.7015 - val_loss: 1.6048 - val_acc: 0.7010\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6198 - acc: 0.7019 - val_loss: 1.6047 - val_acc: 0.7080\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6122 - acc: 0.7016 - val_loss: 1.5891 - val_acc: 0.7060\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6044 - acc: 0.7020 - val_loss: 1.5813 - val_acc: 0.7070\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5962 - acc: 0.7048 - val_loss: 1.5755 - val_acc: 0.7060\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5886 - acc: 0.7039 - val_loss: 1.5679 - val_acc: 0.7060\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5816 - acc: 0.7048 - val_loss: 1.5609 - val_acc: 0.7080\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5739 - acc: 0.7051 - val_loss: 1.5524 - val_acc: 0.7100\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5670 - acc: 0.7047 - val_loss: 1.5454 - val_acc: 0.7080\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5596 - acc: 0.7049 - val_loss: 1.5392 - val_acc: 0.7070\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5523 - acc: 0.7080 - val_loss: 1.5329 - val_acc: 0.7170\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5455 - acc: 0.7063 - val_loss: 1.5254 - val_acc: 0.7150\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5384 - acc: 0.7087 - val_loss: 1.5181 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5314 - acc: 0.7088 - val_loss: 1.5132 - val_acc: 0.7110\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5249 - acc: 0.7084 - val_loss: 1.5042 - val_acc: 0.7140\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5176 - acc: 0.7093 - val_loss: 1.5022 - val_acc: 0.7070\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5115 - acc: 0.7108 - val_loss: 1.4923 - val_acc: 0.7130\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5045 - acc: 0.7117 - val_loss: 1.4951 - val_acc: 0.7090\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4983 - acc: 0.7115 - val_loss: 1.4801 - val_acc: 0.7140\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4912 - acc: 0.7127 - val_loss: 1.4728 - val_acc: 0.7140\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4852 - acc: 0.7104 - val_loss: 1.4667 - val_acc: 0.7170\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4784 - acc: 0.7136 - val_loss: 1.4594 - val_acc: 0.7080\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4724 - acc: 0.7137 - val_loss: 1.4551 - val_acc: 0.7140\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4658 - acc: 0.7149 - val_loss: 1.4498 - val_acc: 0.7160\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4602 - acc: 0.7151 - val_loss: 1.4421 - val_acc: 0.7170\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4538 - acc: 0.7163 - val_loss: 1.4346 - val_acc: 0.7170\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4470 - acc: 0.7149 - val_loss: 1.4340 - val_acc: 0.7200\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4414 - acc: 0.7152 - val_loss: 1.4272 - val_acc: 0.7180\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4362 - acc: 0.7188 - val_loss: 1.4224 - val_acc: 0.7150\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4301 - acc: 0.7153 - val_loss: 1.4136 - val_acc: 0.7190\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4246 - acc: 0.7169 - val_loss: 1.4046 - val_acc: 0.7200\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4182 - acc: 0.7196 - val_loss: 1.4068 - val_acc: 0.7190\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4132 - acc: 0.7200 - val_loss: 1.3998 - val_acc: 0.7190\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4072 - acc: 0.7176 - val_loss: 1.3891 - val_acc: 0.7180\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4016 - acc: 0.7204 - val_loss: 1.3859 - val_acc: 0.7220\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3966 - acc: 0.7208 - val_loss: 1.3825 - val_acc: 0.7210\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3906 - acc: 0.7225 - val_loss: 1.3804 - val_acc: 0.7230\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3854 - acc: 0.7219 - val_loss: 1.3692 - val_acc: 0.7220\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3802 - acc: 0.7215 - val_loss: 1.3651 - val_acc: 0.7200\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3750 - acc: 0.7201 - val_loss: 1.3593 - val_acc: 0.7210\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3703 - acc: 0.7241 - val_loss: 1.3563 - val_acc: 0.7220\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3650 - acc: 0.7241 - val_loss: 1.3489 - val_acc: 0.7250\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3596 - acc: 0.7249 - val_loss: 1.3460 - val_acc: 0.7250\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3543 - acc: 0.7255 - val_loss: 1.3375 - val_acc: 0.7250\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3497 - acc: 0.7248 - val_loss: 1.3340 - val_acc: 0.7270\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3450 - acc: 0.7249 - val_loss: 1.3339 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3404 - acc: 0.7247 - val_loss: 1.3267 - val_acc: 0.7210\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3356 - acc: 0.7267 - val_loss: 1.3237 - val_acc: 0.7160\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3309 - acc: 0.7299 - val_loss: 1.3153 - val_acc: 0.7290\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3262 - acc: 0.7273 - val_loss: 1.3085 - val_acc: 0.7270\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3214 - acc: 0.7275 - val_loss: 1.3051 - val_acc: 0.7210\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3167 - acc: 0.7279 - val_loss: 1.3032 - val_acc: 0.7250\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3118 - acc: 0.7288 - val_loss: 1.2985 - val_acc: 0.7240\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3073 - acc: 0.7301 - val_loss: 1.2923 - val_acc: 0.7240\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3030 - acc: 0.7299 - val_loss: 1.2887 - val_acc: 0.7270\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2984 - acc: 0.7301 - val_loss: 1.2880 - val_acc: 0.7230\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2947 - acc: 0.7303 - val_loss: 1.2849 - val_acc: 0.7180\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2899 - acc: 0.7319 - val_loss: 1.2752 - val_acc: 0.7220\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2858 - acc: 0.7301 - val_loss: 1.2707 - val_acc: 0.7260\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2819 - acc: 0.7319 - val_loss: 1.2682 - val_acc: 0.7240\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2771 - acc: 0.7319 - val_loss: 1.2628 - val_acc: 0.7270\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2734 - acc: 0.7331 - val_loss: 1.2590 - val_acc: 0.7290\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2691 - acc: 0.7335 - val_loss: 1.2582 - val_acc: 0.7250\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2655 - acc: 0.7348 - val_loss: 1.2505 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2614 - acc: 0.7351 - val_loss: 1.2478 - val_acc: 0.7320\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2575 - acc: 0.7325 - val_loss: 1.2434 - val_acc: 0.7270\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2538 - acc: 0.7355 - val_loss: 1.2403 - val_acc: 0.7290\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2497 - acc: 0.7355 - val_loss: 1.2362 - val_acc: 0.7290\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2456 - acc: 0.7341 - val_loss: 1.2344 - val_acc: 0.7310\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2418 - acc: 0.7356 - val_loss: 1.2292 - val_acc: 0.7350\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2380 - acc: 0.7364 - val_loss: 1.2267 - val_acc: 0.7320\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2344 - acc: 0.7365 - val_loss: 1.2225 - val_acc: 0.7280\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2306 - acc: 0.7371 - val_loss: 1.2170 - val_acc: 0.7280\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2272 - acc: 0.7349 - val_loss: 1.2162 - val_acc: 0.7320\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2233 - acc: 0.7385 - val_loss: 1.2114 - val_acc: 0.7290\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2199 - acc: 0.7385 - val_loss: 1.2098 - val_acc: 0.7310\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2166 - acc: 0.7379 - val_loss: 1.2034 - val_acc: 0.7340\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2128 - acc: 0.7393 - val_loss: 1.2085 - val_acc: 0.7230\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2096 - acc: 0.7389 - val_loss: 1.1977 - val_acc: 0.7310\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2066 - acc: 0.7391 - val_loss: 1.1958 - val_acc: 0.7320\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2030 - acc: 0.7389 - val_loss: 1.1909 - val_acc: 0.7340\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1992 - acc: 0.7396 - val_loss: 1.1875 - val_acc: 0.7350\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1969 - acc: 0.7380 - val_loss: 1.1904 - val_acc: 0.7290\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1932 - acc: 0.7395 - val_loss: 1.1811 - val_acc: 0.7340\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1896 - acc: 0.7405 - val_loss: 1.1802 - val_acc: 0.7370\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1861 - acc: 0.7408 - val_loss: 1.1754 - val_acc: 0.7300\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1834 - acc: 0.7409 - val_loss: 1.1755 - val_acc: 0.7320\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1803 - acc: 0.7425 - val_loss: 1.1680 - val_acc: 0.7360\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1766 - acc: 0.7424 - val_loss: 1.1751 - val_acc: 0.7280\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1752 - acc: 0.7417 - val_loss: 1.1609 - val_acc: 0.7340\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1716 - acc: 0.7423 - val_loss: 1.1612 - val_acc: 0.7360\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1687 - acc: 0.7409 - val_loss: 1.1594 - val_acc: 0.7370\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1658 - acc: 0.7441 - val_loss: 1.1571 - val_acc: 0.7350\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1629 - acc: 0.7419 - val_loss: 1.1572 - val_acc: 0.7390\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1598 - acc: 0.7429 - val_loss: 1.1513 - val_acc: 0.7430\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1577 - acc: 0.7429 - val_loss: 1.1462 - val_acc: 0.7390\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1546 - acc: 0.7427 - val_loss: 1.1453 - val_acc: 0.7410\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1517 - acc: 0.7431 - val_loss: 1.1406 - val_acc: 0.7410\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1490 - acc: 0.7461 - val_loss: 1.1447 - val_acc: 0.7300\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1470 - acc: 0.7456 - val_loss: 1.1377 - val_acc: 0.7410\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1438 - acc: 0.7455 - val_loss: 1.1356 - val_acc: 0.7440\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1415 - acc: 0.7457 - val_loss: 1.1324 - val_acc: 0.7390\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1394 - acc: 0.7433 - val_loss: 1.1291 - val_acc: 0.7370\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1362 - acc: 0.7455 - val_loss: 1.1279 - val_acc: 0.7440\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1352 - acc: 0.7449 - val_loss: 1.1235 - val_acc: 0.7370\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1320 - acc: 0.7447 - val_loss: 1.1211 - val_acc: 0.7380\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1293 - acc: 0.7463 - val_loss: 1.1236 - val_acc: 0.7380\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1279 - acc: 0.7457 - val_loss: 1.1186 - val_acc: 0.7400\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1250 - acc: 0.7467 - val_loss: 1.1250 - val_acc: 0.7390\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1234 - acc: 0.7436 - val_loss: 1.1210 - val_acc: 0.7360\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1214 - acc: 0.7468 - val_loss: 1.1116 - val_acc: 0.7430\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1188 - acc: 0.7488 - val_loss: 1.1138 - val_acc: 0.7380\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1174 - acc: 0.7473 - val_loss: 1.1087 - val_acc: 0.7440\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1145 - acc: 0.7479 - val_loss: 1.1078 - val_acc: 0.7410\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1129 - acc: 0.7463 - val_loss: 1.1084 - val_acc: 0.7450\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1110 - acc: 0.7476 - val_loss: 1.1017 - val_acc: 0.7440\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1081 - acc: 0.7471 - val_loss: 1.1027 - val_acc: 0.7410\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1065 - acc: 0.7489 - val_loss: 1.0996 - val_acc: 0.7380\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1045 - acc: 0.7473 - val_loss: 1.0977 - val_acc: 0.7430\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1028 - acc: 0.7496 - val_loss: 1.1026 - val_acc: 0.7370\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1010 - acc: 0.7475 - val_loss: 1.0954 - val_acc: 0.7370\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0997 - acc: 0.7484 - val_loss: 1.0904 - val_acc: 0.7450\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0972 - acc: 0.7500 - val_loss: 1.0884 - val_acc: 0.7410\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0953 - acc: 0.7503 - val_loss: 1.0877 - val_acc: 0.7400\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0937 - acc: 0.7480 - val_loss: 1.0867 - val_acc: 0.7390\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0922 - acc: 0.7485 - val_loss: 1.0866 - val_acc: 0.7430\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0896 - acc: 0.7497 - val_loss: 1.0832 - val_acc: 0.7440\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0879 - acc: 0.7495 - val_loss: 1.0827 - val_acc: 0.7440\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0863 - acc: 0.7511 - val_loss: 1.0830 - val_acc: 0.7430\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.7515 - val_loss: 1.0847 - val_acc: 0.7440\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0835 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7420\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0813 - acc: 0.7508 - val_loss: 1.0765 - val_acc: 0.7400\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0800 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7480\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0782 - acc: 0.7513 - val_loss: 1.0711 - val_acc: 0.7430\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0765 - acc: 0.7496 - val_loss: 1.0708 - val_acc: 0.7420\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0753 - acc: 0.7517 - val_loss: 1.0696 - val_acc: 0.7420\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0740 - acc: 0.7512 - val_loss: 1.0685 - val_acc: 0.7400\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0720 - acc: 0.7521 - val_loss: 1.0647 - val_acc: 0.7430\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0696 - acc: 0.7523 - val_loss: 1.0644 - val_acc: 0.7400\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0693 - acc: 0.7537 - val_loss: 1.0667 - val_acc: 0.7420\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0681 - acc: 0.7524 - val_loss: 1.0655 - val_acc: 0.7390\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0656 - acc: 0.7537 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0650 - acc: 0.7543 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0624 - acc: 0.7532 - val_loss: 1.0579 - val_acc: 0.7480\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0619 - acc: 0.7535 - val_loss: 1.0581 - val_acc: 0.7390\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0599 - acc: 0.7533 - val_loss: 1.0526 - val_acc: 0.7450\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0584 - acc: 0.7539 - val_loss: 1.0534 - val_acc: 0.7440\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0572 - acc: 0.7547 - val_loss: 1.0521 - val_acc: 0.7430\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0560 - acc: 0.7552 - val_loss: 1.0489 - val_acc: 0.7490\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0544 - acc: 0.7543 - val_loss: 1.0516 - val_acc: 0.7460\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0530 - acc: 0.7552 - val_loss: 1.0571 - val_acc: 0.7400\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0526 - acc: 0.7544 - val_loss: 1.0492 - val_acc: 0.7450\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0509 - acc: 0.7527 - val_loss: 1.0506 - val_acc: 0.7450\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0493 - acc: 0.7540 - val_loss: 1.0441 - val_acc: 0.7420\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0479 - acc: 0.7555 - val_loss: 1.0413 - val_acc: 0.7430\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0465 - acc: 0.7545 - val_loss: 1.0415 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0449 - acc: 0.7561 - val_loss: 1.0418 - val_acc: 0.7440\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0438 - acc: 0.7551 - val_loss: 1.0408 - val_acc: 0.7460\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0421 - acc: 0.7563 - val_loss: 1.0380 - val_acc: 0.7540\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0410 - acc: 0.7564 - val_loss: 1.0371 - val_acc: 0.7440\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0400 - acc: 0.7569 - val_loss: 1.0419 - val_acc: 0.7370\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0390 - acc: 0.7565 - val_loss: 1.0352 - val_acc: 0.7450\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0372 - acc: 0.7571 - val_loss: 1.0390 - val_acc: 0.7400\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0359 - acc: 0.7575 - val_loss: 1.0348 - val_acc: 0.7490\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0357 - acc: 0.7561 - val_loss: 1.0300 - val_acc: 0.7450\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0334 - acc: 0.7564 - val_loss: 1.0296 - val_acc: 0.7510\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0328 - acc: 0.7564 - val_loss: 1.0358 - val_acc: 0.7510\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0323 - acc: 0.7555 - val_loss: 1.0284 - val_acc: 0.7460\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0306 - acc: 0.7579 - val_loss: 1.0279 - val_acc: 0.7490\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0292 - acc: 0.7588 - val_loss: 1.0257 - val_acc: 0.7470\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0285 - acc: 0.7576 - val_loss: 1.0270 - val_acc: 0.7450\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0269 - acc: 0.7560 - val_loss: 1.0248 - val_acc: 0.7420\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0257 - acc: 0.7575 - val_loss: 1.0254 - val_acc: 0.7450\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0244 - acc: 0.7569 - val_loss: 1.0243 - val_acc: 0.7450\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0233 - acc: 0.7599 - val_loss: 1.0241 - val_acc: 0.7460\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0225 - acc: 0.7581 - val_loss: 1.0215 - val_acc: 0.7440\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0215 - acc: 0.7583 - val_loss: 1.0181 - val_acc: 0.7440\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0202 - acc: 0.7609 - val_loss: 1.0187 - val_acc: 0.7470\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0196 - acc: 0.7559 - val_loss: 1.0175 - val_acc: 0.7430\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0179 - acc: 0.7597 - val_loss: 1.0187 - val_acc: 0.7550\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0173 - acc: 0.7612 - val_loss: 1.0198 - val_acc: 0.7440\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0165 - acc: 0.7596 - val_loss: 1.0164 - val_acc: 0.7520\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0160 - acc: 0.7591 - val_loss: 1.0137 - val_acc: 0.7450\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0143 - acc: 0.7608 - val_loss: 1.0162 - val_acc: 0.7460\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0129 - acc: 0.7603 - val_loss: 1.0109 - val_acc: 0.7510\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0121 - acc: 0.7615 - val_loss: 1.0209 - val_acc: 0.7530\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0118 - acc: 0.7607 - val_loss: 1.0087 - val_acc: 0.7530\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0103 - acc: 0.7604 - val_loss: 1.0091 - val_acc: 0.7490\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0092 - acc: 0.7607 - val_loss: 1.0086 - val_acc: 0.7460\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0081 - acc: 0.7613 - val_loss: 1.0151 - val_acc: 0.7430\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0073 - acc: 0.7589 - val_loss: 1.0057 - val_acc: 0.7450\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0056 - acc: 0.7624 - val_loss: 1.0074 - val_acc: 0.7530\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0054 - acc: 0.7617 - val_loss: 1.0050 - val_acc: 0.7430\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0038 - acc: 0.7629 - val_loss: 1.0060 - val_acc: 0.7520\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0037 - acc: 0.7595 - val_loss: 1.0063 - val_acc: 0.7470\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0023 - acc: 0.7624 - val_loss: 1.0027 - val_acc: 0.7550\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0012 - acc: 0.7624 - val_loss: 1.0099 - val_acc: 0.7470\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0015 - acc: 0.7605 - val_loss: 1.0015 - val_acc: 0.7520\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9996 - acc: 0.7640 - val_loss: 0.9987 - val_acc: 0.7490\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9989 - acc: 0.7629 - val_loss: 0.9976 - val_acc: 0.7500\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9978 - acc: 0.7636 - val_loss: 1.0029 - val_acc: 0.7480\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9971 - acc: 0.7621 - val_loss: 0.9973 - val_acc: 0.7540\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9967 - acc: 0.7612 - val_loss: 0.9938 - val_acc: 0.7510\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9952 - acc: 0.7636 - val_loss: 0.9967 - val_acc: 0.7530\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9945 - acc: 0.7635 - val_loss: 1.0000 - val_acc: 0.7500\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9937 - acc: 0.7629 - val_loss: 0.9950 - val_acc: 0.7470\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9926 - acc: 0.7643 - val_loss: 0.9954 - val_acc: 0.7530\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9927 - acc: 0.7635 - val_loss: 0.9956 - val_acc: 0.7470\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9908 - acc: 0.7643 - val_loss: 0.9973 - val_acc: 0.7450\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9906 - acc: 0.7633 - val_loss: 0.9918 - val_acc: 0.7570\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9895 - acc: 0.7609 - val_loss: 0.9884 - val_acc: 0.7470\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9884 - acc: 0.7655 - val_loss: 0.9925 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9876 - acc: 0.7648 - val_loss: 0.9932 - val_acc: 0.7490\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9868 - acc: 0.7620 - val_loss: 0.9950 - val_acc: 0.7450\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9861 - acc: 0.7667 - val_loss: 0.9908 - val_acc: 0.7520\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9855 - acc: 0.7635 - val_loss: 0.9861 - val_acc: 0.7460\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9847 - acc: 0.7668 - val_loss: 0.9927 - val_acc: 0.7400\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9841 - acc: 0.7645 - val_loss: 0.9864 - val_acc: 0.7560\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9829 - acc: 0.7653 - val_loss: 0.9829 - val_acc: 0.7460\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9823 - acc: 0.7655 - val_loss: 0.9877 - val_acc: 0.7510\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9815 - acc: 0.7645 - val_loss: 0.9833 - val_acc: 0.7570\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9808 - acc: 0.7655 - val_loss: 0.9834 - val_acc: 0.7520\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9789 - acc: 0.7656 - val_loss: 0.9922 - val_acc: 0.7470\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9784 - acc: 0.7665 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9785 - acc: 0.7648 - val_loss: 0.9781 - val_acc: 0.7530\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9774 - acc: 0.7657 - val_loss: 0.9808 - val_acc: 0.7510\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9768 - acc: 0.7657 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9760 - acc: 0.7667 - val_loss: 0.9817 - val_acc: 0.7510\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9747 - acc: 0.7656 - val_loss: 0.9809 - val_acc: 0.7530\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9749 - acc: 0.7655 - val_loss: 0.9872 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9742 - acc: 0.7675 - val_loss: 0.9758 - val_acc: 0.7460\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9733 - acc: 0.7692 - val_loss: 0.9749 - val_acc: 0.7570\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9723 - acc: 0.7675 - val_loss: 0.9831 - val_acc: 0.7460\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9721 - acc: 0.7671 - val_loss: 0.9876 - val_acc: 0.7440\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9712 - acc: 0.7688 - val_loss: 0.9750 - val_acc: 0.7560\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9705 - acc: 0.7691 - val_loss: 0.9740 - val_acc: 0.7520\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9695 - acc: 0.7700 - val_loss: 0.9739 - val_acc: 0.7550\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9689 - acc: 0.7691 - val_loss: 0.9769 - val_acc: 0.7460\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9683 - acc: 0.7680 - val_loss: 0.9741 - val_acc: 0.7440\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9672 - acc: 0.7667 - val_loss: 0.9804 - val_acc: 0.7460\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9665 - acc: 0.7671 - val_loss: 0.9692 - val_acc: 0.7480\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9658 - acc: 0.7677 - val_loss: 0.9884 - val_acc: 0.7550\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9668 - acc: 0.7697 - val_loss: 0.9716 - val_acc: 0.7540\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9652 - acc: 0.7655 - val_loss: 0.9746 - val_acc: 0.7520\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9644 - acc: 0.7688 - val_loss: 0.9681 - val_acc: 0.7560\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9641 - acc: 0.7685 - val_loss: 0.9718 - val_acc: 0.7510\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.7688 - val_loss: 0.9729 - val_acc: 0.7540\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9627 - acc: 0.7673 - val_loss: 0.9664 - val_acc: 0.7490\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9621 - acc: 0.7689 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7699 - val_loss: 0.9635 - val_acc: 0.7540\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7688 - val_loss: 0.9703 - val_acc: 0.7440\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9603 - acc: 0.7695 - val_loss: 0.9697 - val_acc: 0.7480\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9605 - acc: 0.7669 - val_loss: 0.9734 - val_acc: 0.7450\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9597 - acc: 0.7693 - val_loss: 0.9655 - val_acc: 0.7500\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9586 - acc: 0.7687 - val_loss: 0.9644 - val_acc: 0.7510\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9577 - acc: 0.7683 - val_loss: 0.9633 - val_acc: 0.7580\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9576 - acc: 0.7681 - val_loss: 0.9751 - val_acc: 0.7540\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9574 - acc: 0.7683 - val_loss: 0.9652 - val_acc: 0.7600\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9572 - acc: 0.7700 - val_loss: 0.9627 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9555 - acc: 0.7677 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9556 - acc: 0.7693 - val_loss: 0.9616 - val_acc: 0.7630\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9555 - acc: 0.7679 - val_loss: 0.9629 - val_acc: 0.7540\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7692 - val_loss: 0.9640 - val_acc: 0.7460\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7700 - val_loss: 0.9772 - val_acc: 0.7480\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9538 - acc: 0.7713 - val_loss: 0.9612 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9530 - acc: 0.7688 - val_loss: 0.9576 - val_acc: 0.7620\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9510 - acc: 0.7699 - val_loss: 0.9588 - val_acc: 0.7510\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9512 - acc: 0.7692 - val_loss: 0.9565 - val_acc: 0.7580\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9517 - acc: 0.7689 - val_loss: 0.9629 - val_acc: 0.7490\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9504 - acc: 0.7700 - val_loss: 0.9594 - val_acc: 0.7610\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9503 - acc: 0.7680 - val_loss: 0.9571 - val_acc: 0.7550\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9496 - acc: 0.7681 - val_loss: 0.9551 - val_acc: 0.7600\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9488 - acc: 0.7712 - val_loss: 0.9659 - val_acc: 0.7510\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9487 - acc: 0.7713 - val_loss: 0.9705 - val_acc: 0.7460\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9483 - acc: 0.7709 - val_loss: 0.9521 - val_acc: 0.7580\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9468 - acc: 0.7705 - val_loss: 0.9535 - val_acc: 0.7620\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9469 - acc: 0.7689 - val_loss: 0.9570 - val_acc: 0.7630\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9463 - acc: 0.7717 - val_loss: 0.9548 - val_acc: 0.7530\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9472 - acc: 0.7719 - val_loss: 0.9545 - val_acc: 0.7610\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9461 - acc: 0.7701 - val_loss: 0.9621 - val_acc: 0.7510\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9448 - acc: 0.7716 - val_loss: 0.9535 - val_acc: 0.7650\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9450 - acc: 0.7705 - val_loss: 0.9510 - val_acc: 0.7590\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9444 - acc: 0.7703 - val_loss: 0.9564 - val_acc: 0.7510\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9434 - acc: 0.7708 - val_loss: 0.9491 - val_acc: 0.7660\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.7720 - val_loss: 0.9536 - val_acc: 0.7530\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9417 - acc: 0.7707 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9429 - acc: 0.7719 - val_loss: 0.9516 - val_acc: 0.7600\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9419 - acc: 0.7729 - val_loss: 0.9532 - val_acc: 0.7620\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9450 - acc: 0.770 - 0s 44us/step - loss: 0.9427 - acc: 0.7716 - val_loss: 0.9465 - val_acc: 0.7610\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9407 - acc: 0.7715 - val_loss: 0.9475 - val_acc: 0.7570\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9408 - acc: 0.7723 - val_loss: 0.9450 - val_acc: 0.7630\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9394 - acc: 0.7712 - val_loss: 0.9461 - val_acc: 0.7660\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9392 - acc: 0.7729 - val_loss: 0.9468 - val_acc: 0.7550\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9388 - acc: 0.7712 - val_loss: 0.9449 - val_acc: 0.7540\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9387 - acc: 0.7731 - val_loss: 0.9479 - val_acc: 0.7640\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9381 - acc: 0.7711 - val_loss: 0.9654 - val_acc: 0.7400\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9390 - acc: 0.7708 - val_loss: 0.9449 - val_acc: 0.7600\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9373 - acc: 0.7705 - val_loss: 0.9459 - val_acc: 0.7630\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9374 - acc: 0.7713 - val_loss: 0.9447 - val_acc: 0.7660\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9356 - acc: 0.7717 - val_loss: 0.9426 - val_acc: 0.7650\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7747 - val_loss: 0.9450 - val_acc: 0.7640\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9352 - acc: 0.7708 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9345 - acc: 0.7724 - val_loss: 0.9514 - val_acc: 0.7500\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7739 - val_loss: 0.9410 - val_acc: 0.7650\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9336 - acc: 0.7701 - val_loss: 0.9415 - val_acc: 0.7610\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9341 - acc: 0.7712 - val_loss: 0.9454 - val_acc: 0.7580\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9335 - acc: 0.7724 - val_loss: 0.9444 - val_acc: 0.7510\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9326 - acc: 0.7729 - val_loss: 0.9418 - val_acc: 0.7610\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9327 - acc: 0.7732 - val_loss: 0.9391 - val_acc: 0.7640\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9313 - acc: 0.7736 - val_loss: 0.9389 - val_acc: 0.7680\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9310 - acc: 0.7736 - val_loss: 0.9373 - val_acc: 0.7640\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9310 - acc: 0.7708 - val_loss: 0.9374 - val_acc: 0.7620\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9302 - acc: 0.7748 - val_loss: 0.9408 - val_acc: 0.7600\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9297 - acc: 0.7739 - val_loss: 0.9381 - val_acc: 0.7620\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9301 - acc: 0.7727 - val_loss: 0.9401 - val_acc: 0.7540\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9296 - acc: 0.7729 - val_loss: 0.9441 - val_acc: 0.7550\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9287 - acc: 0.7724 - val_loss: 0.9380 - val_acc: 0.7680\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9398 - val_acc: 0.7630\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9389 - val_acc: 0.7570\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9277 - acc: 0.7737 - val_loss: 0.9360 - val_acc: 0.7630\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9273 - acc: 0.7733 - val_loss: 0.9399 - val_acc: 0.7600\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9268 - acc: 0.7725 - val_loss: 0.9360 - val_acc: 0.7590\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9270 - acc: 0.7736 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9268 - acc: 0.7729 - val_loss: 0.9420 - val_acc: 0.7660\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.7745 - val_loss: 0.9486 - val_acc: 0.7580\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9260 - acc: 0.7741 - val_loss: 0.9360 - val_acc: 0.7610\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.7737 - val_loss: 0.9329 - val_acc: 0.7590\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9237 - acc: 0.7745 - val_loss: 0.9386 - val_acc: 0.7650\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9235 - acc: 0.7752 - val_loss: 0.9405 - val_acc: 0.7580\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9235 - acc: 0.7735 - val_loss: 0.9440 - val_acc: 0.7600\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9241 - acc: 0.7725 - val_loss: 0.9371 - val_acc: 0.7580\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9232 - acc: 0.7736 - val_loss: 0.9312 - val_acc: 0.7650\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9228 - acc: 0.7744 - val_loss: 0.9327 - val_acc: 0.7630\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9222 - acc: 0.7721 - val_loss: 0.9306 - val_acc: 0.7700\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9211 - acc: 0.7735 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9210 - acc: 0.7735 - val_loss: 0.9288 - val_acc: 0.7670\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9204 - acc: 0.7720 - val_loss: 0.9274 - val_acc: 0.7680\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9204 - acc: 0.7763 - val_loss: 0.9447 - val_acc: 0.7620\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7756 - val_loss: 0.9429 - val_acc: 0.7510\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9194 - acc: 0.7733 - val_loss: 0.9497 - val_acc: 0.7530\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7737 - val_loss: 0.9272 - val_acc: 0.7650\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9191 - acc: 0.7760 - val_loss: 0.9390 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9193 - acc: 0.7736 - val_loss: 0.9308 - val_acc: 0.7650\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9188 - acc: 0.7751 - val_loss: 0.9294 - val_acc: 0.7620\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9179 - acc: 0.7748 - val_loss: 0.9351 - val_acc: 0.7590\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9184 - acc: 0.7747 - val_loss: 0.9257 - val_acc: 0.7670\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9167 - acc: 0.7748 - val_loss: 0.9266 - val_acc: 0.7690\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9168 - acc: 0.7740 - val_loss: 0.9312 - val_acc: 0.7640\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9165 - acc: 0.7756 - val_loss: 0.9301 - val_acc: 0.7620\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9163 - acc: 0.7763 - val_loss: 0.9313 - val_acc: 0.7630\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9161 - acc: 0.7743 - val_loss: 0.9274 - val_acc: 0.7700\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9154 - acc: 0.7759 - val_loss: 0.9281 - val_acc: 0.7530\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9152 - acc: 0.7751 - val_loss: 0.9285 - val_acc: 0.7660\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9149 - acc: 0.7756 - val_loss: 0.9289 - val_acc: 0.7560\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9145 - acc: 0.7756 - val_loss: 0.9243 - val_acc: 0.7690\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9138 - acc: 0.7768 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9137 - acc: 0.7743 - val_loss: 0.9252 - val_acc: 0.7630\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9131 - acc: 0.7760 - val_loss: 0.9238 - val_acc: 0.7650\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9133 - acc: 0.7751 - val_loss: 0.9230 - val_acc: 0.7660\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9129 - acc: 0.7724 - val_loss: 0.9237 - val_acc: 0.7600\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9137 - acc: 0.7729 - val_loss: 0.9236 - val_acc: 0.7670\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9113 - acc: 0.7757 - val_loss: 0.9300 - val_acc: 0.7560\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9125 - acc: 0.7745 - val_loss: 0.9280 - val_acc: 0.7640\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9113 - acc: 0.7727 - val_loss: 0.9250 - val_acc: 0.7630\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9114 - acc: 0.7753 - val_loss: 0.9233 - val_acc: 0.7580\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9115 - acc: 0.7728 - val_loss: 0.9228 - val_acc: 0.7650\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9119 - acc: 0.7747 - val_loss: 0.9248 - val_acc: 0.7570\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9101 - acc: 0.7753 - val_loss: 0.9370 - val_acc: 0.7540\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9103 - acc: 0.7777 - val_loss: 0.9350 - val_acc: 0.7510\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9098 - acc: 0.7752 - val_loss: 0.9286 - val_acc: 0.7580\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9102 - acc: 0.7740 - val_loss: 0.9217 - val_acc: 0.7650\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9085 - acc: 0.7751 - val_loss: 0.9227 - val_acc: 0.7610\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9097 - acc: 0.7771 - val_loss: 0.9202 - val_acc: 0.7650\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9076 - acc: 0.7768 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9082 - acc: 0.7761 - val_loss: 0.9350 - val_acc: 0.7640\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9085 - acc: 0.7753 - val_loss: 0.9177 - val_acc: 0.7690\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9064 - acc: 0.7777 - val_loss: 0.9183 - val_acc: 0.7660\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9064 - acc: 0.7748 - val_loss: 0.9190 - val_acc: 0.7610\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9062 - acc: 0.7765 - val_loss: 0.9218 - val_acc: 0.7620\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9070 - acc: 0.7749 - val_loss: 0.9217 - val_acc: 0.7680\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9063 - acc: 0.7764 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9058 - acc: 0.7761 - val_loss: 0.9189 - val_acc: 0.7640\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9059 - acc: 0.7767 - val_loss: 0.9195 - val_acc: 0.7640\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9049 - acc: 0.7780 - val_loss: 0.9222 - val_acc: 0.7580\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9043 - acc: 0.7783 - val_loss: 0.9232 - val_acc: 0.7610\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9049 - acc: 0.7772 - val_loss: 0.9200 - val_acc: 0.7640\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9046 - acc: 0.7784 - val_loss: 0.9249 - val_acc: 0.7580\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9037 - acc: 0.7787 - val_loss: 0.9218 - val_acc: 0.7600\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9035 - acc: 0.7779 - val_loss: 0.9174 - val_acc: 0.7690\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9032 - acc: 0.7773 - val_loss: 0.9202 - val_acc: 0.7660\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7767 - val_loss: 0.9192 - val_acc: 0.7660\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7755 - val_loss: 0.9171 - val_acc: 0.7670\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9024 - acc: 0.7784 - val_loss: 0.9144 - val_acc: 0.7670\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9019 - acc: 0.7775 - val_loss: 0.9148 - val_acc: 0.7670\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9014 - acc: 0.7773 - val_loss: 0.9198 - val_acc: 0.7520\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9014 - acc: 0.7776 - val_loss: 0.9144 - val_acc: 0.7610\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9016 - acc: 0.7767 - val_loss: 0.9141 - val_acc: 0.7670\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9006 - acc: 0.7781 - val_loss: 0.9152 - val_acc: 0.7620\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9012 - acc: 0.7745 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8994 - acc: 0.7773 - val_loss: 0.9142 - val_acc: 0.7640\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.7777 - val_loss: 0.9187 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9009 - acc: 0.7785 - val_loss: 0.9140 - val_acc: 0.7620\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8999 - acc: 0.7775 - val_loss: 0.9120 - val_acc: 0.7670\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8987 - acc: 0.7792 - val_loss: 0.9345 - val_acc: 0.7490\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9005 - acc: 0.7776 - val_loss: 0.9133 - val_acc: 0.7700\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8992 - acc: 0.7785 - val_loss: 0.9191 - val_acc: 0.7580\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8987 - acc: 0.7788 - val_loss: 0.9105 - val_acc: 0.7640\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8982 - acc: 0.7780 - val_loss: 0.9209 - val_acc: 0.7580\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8986 - acc: 0.7792 - val_loss: 0.9102 - val_acc: 0.7650\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8975 - acc: 0.7781 - val_loss: 0.9128 - val_acc: 0.7690\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8969 - acc: 0.7769 - val_loss: 0.9133 - val_acc: 0.7560\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8982 - acc: 0.7776 - val_loss: 0.9196 - val_acc: 0.7680\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8989 - acc: 0.7796 - val_loss: 0.9117 - val_acc: 0.7690\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8973 - acc: 0.7791 - val_loss: 0.9126 - val_acc: 0.7600\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.7781 - val_loss: 0.9115 - val_acc: 0.7590\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8961 - acc: 0.7791 - val_loss: 0.9100 - val_acc: 0.7660\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8964 - acc: 0.7771 - val_loss: 0.9090 - val_acc: 0.7660\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8956 - acc: 0.7788 - val_loss: 0.9136 - val_acc: 0.7560\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8959 - acc: 0.7793 - val_loss: 0.9222 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8955 - acc: 0.7792 - val_loss: 0.9107 - val_acc: 0.7670\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8950 - acc: 0.7809 - val_loss: 0.9091 - val_acc: 0.7700\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8940 - acc: 0.7764 - val_loss: 0.9135 - val_acc: 0.7680\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8941 - acc: 0.7785 - val_loss: 0.9099 - val_acc: 0.7600\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8928 - acc: 0.7781 - val_loss: 0.9082 - val_acc: 0.7660\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7785 - val_loss: 0.9113 - val_acc: 0.7620\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8926 - acc: 0.7807 - val_loss: 0.9074 - val_acc: 0.7710\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8939 - acc: 0.7772 - val_loss: 0.9051 - val_acc: 0.7680\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8925 - acc: 0.7781 - val_loss: 0.9156 - val_acc: 0.7650\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7793 - val_loss: 0.9136 - val_acc: 0.7660\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7797 - val_loss: 0.9077 - val_acc: 0.7670\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8929 - acc: 0.7780 - val_loss: 0.9101 - val_acc: 0.7660\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8921 - acc: 0.7788 - val_loss: 0.9118 - val_acc: 0.7580\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8917 - acc: 0.7781 - val_loss: 0.9090 - val_acc: 0.7700\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7797 - val_loss: 0.9057 - val_acc: 0.7640\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8922 - acc: 0.7783 - val_loss: 0.9152 - val_acc: 0.7520\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8911 - acc: 0.7776 - val_loss: 0.9081 - val_acc: 0.7620\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8917 - acc: 0.7783 - val_loss: 0.9075 - val_acc: 0.7630\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7773 - val_loss: 0.9061 - val_acc: 0.7660\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8903 - acc: 0.7791 - val_loss: 0.9047 - val_acc: 0.7680\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8894 - acc: 0.7792 - val_loss: 0.9070 - val_acc: 0.7620\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8905 - acc: 0.7783 - val_loss: 0.9043 - val_acc: 0.7690\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8899 - acc: 0.7793 - val_loss: 0.9092 - val_acc: 0.7670\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8912 - acc: 0.7788 - val_loss: 0.9194 - val_acc: 0.7610\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8890 - acc: 0.7799 - val_loss: 0.9028 - val_acc: 0.7710\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8892 - acc: 0.7804 - val_loss: 0.9060 - val_acc: 0.7610\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8895 - acc: 0.7793 - val_loss: 0.9112 - val_acc: 0.7640\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8895 - acc: 0.7792 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8881 - acc: 0.7779 - val_loss: 0.9022 - val_acc: 0.7650\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8873 - acc: 0.7775 - val_loss: 0.9023 - val_acc: 0.7700\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8875 - acc: 0.7791 - val_loss: 0.9098 - val_acc: 0.7650\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8873 - acc: 0.7819 - val_loss: 0.8998 - val_acc: 0.7700\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8862 - acc: 0.7803 - val_loss: 0.9126 - val_acc: 0.7620\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8866 - acc: 0.7807 - val_loss: 0.9049 - val_acc: 0.7710\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8878 - acc: 0.7803 - val_loss: 0.9050 - val_acc: 0.7620\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8868 - acc: 0.7817 - val_loss: 0.9017 - val_acc: 0.7600\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8861 - acc: 0.7813 - val_loss: 0.9029 - val_acc: 0.7710\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8866 - acc: 0.7809 - val_loss: 0.9049 - val_acc: 0.7630\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8865 - acc: 0.7808 - val_loss: 0.9038 - val_acc: 0.7690\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8863 - acc: 0.7809 - val_loss: 0.9004 - val_acc: 0.7650\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8860 - acc: 0.7809 - val_loss: 0.9061 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8848 - acc: 0.7819 - val_loss: 0.9010 - val_acc: 0.7680\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8845 - acc: 0.7815 - val_loss: 0.9010 - val_acc: 0.7630\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8846 - acc: 0.7801 - val_loss: 0.9220 - val_acc: 0.7600\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.7803 - val_loss: 0.9060 - val_acc: 0.7660\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8851 - acc: 0.7827 - val_loss: 0.9006 - val_acc: 0.7710\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8846 - acc: 0.7805 - val_loss: 0.9013 - val_acc: 0.7700\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8848 - acc: 0.7805 - val_loss: 0.9001 - val_acc: 0.7680\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8849 - acc: 0.7813 - val_loss: 0.8981 - val_acc: 0.7710\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7795 - val_loss: 0.9019 - val_acc: 0.7670\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8833 - acc: 0.7808 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7808 - val_loss: 0.8989 - val_acc: 0.7600\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8830 - acc: 0.7813 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8840 - acc: 0.7820 - val_loss: 0.9038 - val_acc: 0.7600\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8820 - acc: 0.7812 - val_loss: 0.9092 - val_acc: 0.7610\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8823 - acc: 0.7809 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8825 - acc: 0.7797 - val_loss: 0.9331 - val_acc: 0.7580\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8835 - acc: 0.7816 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8814 - acc: 0.7823 - val_loss: 0.9037 - val_acc: 0.7570\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8822 - acc: 0.7815 - val_loss: 0.8956 - val_acc: 0.7760\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8825 - acc: 0.7803 - val_loss: 0.9003 - val_acc: 0.7600\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8818 - acc: 0.7804 - val_loss: 0.8978 - val_acc: 0.7700\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8806 - acc: 0.7816 - val_loss: 0.8948 - val_acc: 0.7750\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8805 - acc: 0.7829 - val_loss: 0.9006 - val_acc: 0.7690\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8809 - acc: 0.7815 - val_loss: 0.8999 - val_acc: 0.7710\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8807 - acc: 0.7827 - val_loss: 0.8971 - val_acc: 0.7670\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8810 - acc: 0.7813 - val_loss: 0.8964 - val_acc: 0.7720\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8806 - acc: 0.7815 - val_loss: 0.9139 - val_acc: 0.7570\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8803 - acc: 0.7831 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8803 - acc: 0.7825 - val_loss: 0.9025 - val_acc: 0.7760\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8799 - acc: 0.7827 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8796 - acc: 0.7824 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8794 - acc: 0.7823 - val_loss: 0.8961 - val_acc: 0.7670\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8789 - acc: 0.781 - 0s 36us/step - loss: 0.8798 - acc: 0.7816 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8789 - acc: 0.7817 - val_loss: 0.9079 - val_acc: 0.7670\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8780 - acc: 0.7828 - val_loss: 0.8931 - val_acc: 0.7660\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8794 - acc: 0.7817 - val_loss: 0.9093 - val_acc: 0.7570\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8796 - acc: 0.7829 - val_loss: 0.9164 - val_acc: 0.7620\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8798 - acc: 0.7803 - val_loss: 0.8987 - val_acc: 0.7660\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8778 - acc: 0.7827 - val_loss: 0.9133 - val_acc: 0.7520\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8782 - acc: 0.7827 - val_loss: 0.8960 - val_acc: 0.7670\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8773 - acc: 0.7812 - val_loss: 0.9114 - val_acc: 0.7610\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8788 - acc: 0.7813 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8767 - acc: 0.7835 - val_loss: 0.9009 - val_acc: 0.7680\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8770 - acc: 0.7852 - val_loss: 0.8958 - val_acc: 0.7730\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8760 - acc: 0.7825 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8765 - acc: 0.7804 - val_loss: 0.8937 - val_acc: 0.7740\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8771 - acc: 0.7832 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8775 - acc: 0.7815 - val_loss: 0.8947 - val_acc: 0.7720\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8756 - acc: 0.7839 - val_loss: 0.8932 - val_acc: 0.7710\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8765 - acc: 0.7823 - val_loss: 0.8949 - val_acc: 0.7690\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8767 - acc: 0.7823 - val_loss: 0.8966 - val_acc: 0.7650\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8755 - acc: 0.7831 - val_loss: 0.8939 - val_acc: 0.7660\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8768 - acc: 0.7817 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8773 - acc: 0.7820 - val_loss: 0.8954 - val_acc: 0.7690\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8759 - acc: 0.7800 - val_loss: 0.9110 - val_acc: 0.7630\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8749 - acc: 0.7825 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8749 - acc: 0.7833 - val_loss: 0.8952 - val_acc: 0.7670\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.7833 - val_loss: 0.8930 - val_acc: 0.7680\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8733 - acc: 0.7859 - val_loss: 0.8997 - val_acc: 0.7570\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8744 - acc: 0.7843 - val_loss: 0.8956 - val_acc: 0.7630\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8740 - acc: 0.7827 - val_loss: 0.8945 - val_acc: 0.7690\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8744 - acc: 0.7829 - val_loss: 0.8928 - val_acc: 0.7660\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8747 - acc: 0.7817 - val_loss: 0.8969 - val_acc: 0.7700\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8738 - acc: 0.7829 - val_loss: 0.8980 - val_acc: 0.7670\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8726 - acc: 0.7845 - val_loss: 0.9185 - val_acc: 0.7620\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8742 - acc: 0.7841 - val_loss: 0.8928 - val_acc: 0.7720\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8733 - acc: 0.7853 - val_loss: 0.8896 - val_acc: 0.7700\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8725 - acc: 0.7813 - val_loss: 0.8941 - val_acc: 0.7650\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8729 - acc: 0.7840 - val_loss: 0.9022 - val_acc: 0.7620\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8732 - acc: 0.7815 - val_loss: 0.8955 - val_acc: 0.7600\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8727 - acc: 0.7836 - val_loss: 0.8885 - val_acc: 0.7710\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8718 - acc: 0.7837 - val_loss: 0.8904 - val_acc: 0.7700\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8715 - acc: 0.7837 - val_loss: 0.8936 - val_acc: 0.7660\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8722 - acc: 0.7815 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8735 - acc: 0.7813 - val_loss: 0.8924 - val_acc: 0.7670\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7855 - val_loss: 0.8929 - val_acc: 0.7710\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7835 - val_loss: 0.8895 - val_acc: 0.7740\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7851 - val_loss: 0.9019 - val_acc: 0.7520\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8706 - acc: 0.7836 - val_loss: 0.8904 - val_acc: 0.7680\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7833 - val_loss: 0.8919 - val_acc: 0.7690\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7847 - val_loss: 0.8945 - val_acc: 0.7730\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8701 - acc: 0.7823 - val_loss: 0.8916 - val_acc: 0.7700\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7828 - val_loss: 0.8978 - val_acc: 0.7550\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8715 - acc: 0.7839 - val_loss: 0.9058 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8698 - acc: 0.7824 - val_loss: 0.8964 - val_acc: 0.7680\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8692 - acc: 0.7851 - val_loss: 0.8976 - val_acc: 0.7690\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7844 - val_loss: 0.8921 - val_acc: 0.7620\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8695 - acc: 0.7840 - val_loss: 0.8903 - val_acc: 0.7720\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7839 - val_loss: 0.8968 - val_acc: 0.7590\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8697 - acc: 0.7833 - val_loss: 0.8901 - val_acc: 0.7650\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8685 - acc: 0.7851 - val_loss: 0.8876 - val_acc: 0.7730\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8696 - acc: 0.7837 - val_loss: 0.9169 - val_acc: 0.7610\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8693 - acc: 0.7831 - val_loss: 0.8896 - val_acc: 0.7770\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8682 - acc: 0.7828 - val_loss: 0.8882 - val_acc: 0.7670\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8690 - acc: 0.7803 - val_loss: 0.8892 - val_acc: 0.7680\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8687 - acc: 0.7824 - val_loss: 0.8875 - val_acc: 0.7770\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8687 - acc: 0.7839 - val_loss: 0.8971 - val_acc: 0.7640\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8680 - acc: 0.7848 - val_loss: 0.8876 - val_acc: 0.7740\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8674 - acc: 0.7847 - val_loss: 0.9123 - val_acc: 0.7540\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8684 - acc: 0.7851 - val_loss: 0.8878 - val_acc: 0.7690\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8683 - acc: 0.7845 - val_loss: 0.8944 - val_acc: 0.7520\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8670 - acc: 0.7845 - val_loss: 0.8923 - val_acc: 0.7690\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8683 - acc: 0.7837 - val_loss: 0.8988 - val_acc: 0.7670\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8667 - acc: 0.7852 - val_loss: 0.8988 - val_acc: 0.7700\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8675 - acc: 0.7851 - val_loss: 0.8952 - val_acc: 0.7700\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7843 - val_loss: 0.8854 - val_acc: 0.7690\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7835 - val_loss: 0.8869 - val_acc: 0.7720\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8668 - acc: 0.7824 - val_loss: 0.9022 - val_acc: 0.7690\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8677 - acc: 0.7831 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8654 - acc: 0.7859 - val_loss: 0.8909 - val_acc: 0.7730\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8671 - acc: 0.7841 - val_loss: 0.8867 - val_acc: 0.7720\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8649 - acc: 0.7823 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8665 - acc: 0.7848 - val_loss: 0.8929 - val_acc: 0.7740\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8648 - acc: 0.7843 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8660 - acc: 0.7836 - val_loss: 0.8881 - val_acc: 0.7660\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7864 - val_loss: 0.8970 - val_acc: 0.7670\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8653 - acc: 0.7841 - val_loss: 0.8851 - val_acc: 0.7760\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7836 - val_loss: 0.8896 - val_acc: 0.7760\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8640 - acc: 0.7841 - val_loss: 0.9073 - val_acc: 0.7550\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8668 - acc: 0.7856 - val_loss: 0.8923 - val_acc: 0.7630\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8658 - acc: 0.7857 - val_loss: 0.8906 - val_acc: 0.7760\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8642 - acc: 0.7855 - val_loss: 0.8862 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7841 - val_loss: 0.8937 - val_acc: 0.7660\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8647 - acc: 0.7861 - val_loss: 0.9076 - val_acc: 0.7620\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8655 - acc: 0.7853 - val_loss: 0.8893 - val_acc: 0.7700\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8645 - acc: 0.7832 - val_loss: 0.8870 - val_acc: 0.7620\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8646 - acc: 0.7843 - val_loss: 0.8907 - val_acc: 0.7620\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7856 - val_loss: 0.8835 - val_acc: 0.7760\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8632 - acc: 0.7845 - val_loss: 0.8857 - val_acc: 0.7690\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8629 - acc: 0.7832 - val_loss: 0.8880 - val_acc: 0.7660\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8625 - acc: 0.7867 - val_loss: 0.8946 - val_acc: 0.7670\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8632 - acc: 0.7867 - val_loss: 0.9199 - val_acc: 0.7470\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8648 - acc: 0.7844 - val_loss: 0.8890 - val_acc: 0.7710\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8626 - acc: 0.7867 - val_loss: 0.8937 - val_acc: 0.7620\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8642 - acc: 0.7847 - val_loss: 0.8906 - val_acc: 0.7690\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8622 - acc: 0.7860 - val_loss: 0.8846 - val_acc: 0.7710\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8621 - acc: 0.7855 - val_loss: 0.8870 - val_acc: 0.7670\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8630 - acc: 0.7855 - val_loss: 0.8827 - val_acc: 0.7710\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8619 - acc: 0.7847 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8638 - acc: 0.7860 - val_loss: 0.8872 - val_acc: 0.7560\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8615 - acc: 0.7848 - val_loss: 0.8840 - val_acc: 0.7770\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8606 - acc: 0.7865 - val_loss: 0.8842 - val_acc: 0.7670\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8600 - acc: 0.7875 - val_loss: 0.8842 - val_acc: 0.7740\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8627 - acc: 0.7840 - val_loss: 0.8854 - val_acc: 0.7720\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8617 - acc: 0.7847 - val_loss: 0.8840 - val_acc: 0.7750\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8607 - acc: 0.7855 - val_loss: 0.8852 - val_acc: 0.7750\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8610 - acc: 0.7844 - val_loss: 0.8845 - val_acc: 0.7740\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8604 - acc: 0.7869 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8603 - acc: 0.7865 - val_loss: 0.8844 - val_acc: 0.7720\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8603 - acc: 0.7857 - val_loss: 0.8933 - val_acc: 0.7670\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8608 - acc: 0.7860 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8609 - acc: 0.7847 - val_loss: 0.8915 - val_acc: 0.7640\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8605 - acc: 0.7871 - val_loss: 0.8904 - val_acc: 0.7600\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8605 - acc: 0.7867 - val_loss: 0.8816 - val_acc: 0.7760\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8610 - acc: 0.7833 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8585 - acc: 0.7853 - val_loss: 0.8882 - val_acc: 0.7700\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8602 - acc: 0.7852 - val_loss: 0.8810 - val_acc: 0.7760\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8584 - acc: 0.7859 - val_loss: 0.8847 - val_acc: 0.7660\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8604 - acc: 0.7852 - val_loss: 0.8929 - val_acc: 0.7670\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8613 - acc: 0.7852 - val_loss: 0.8826 - val_acc: 0.7730\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8590 - acc: 0.7857 - val_loss: 0.8836 - val_acc: 0.7750\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8594 - acc: 0.7865 - val_loss: 0.8780 - val_acc: 0.7750\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8584 - acc: 0.7837 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8569 - acc: 0.7856 - val_loss: 0.9019 - val_acc: 0.7580\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8833 - val_acc: 0.7680\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8587 - acc: 0.7867 - val_loss: 0.8867 - val_acc: 0.7670\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8589 - acc: 0.7884 - val_loss: 0.8854 - val_acc: 0.7630\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8580 - acc: 0.7863 - val_loss: 0.8930 - val_acc: 0.7600\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8877 - val_acc: 0.7720\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8589 - acc: 0.7864 - val_loss: 0.8848 - val_acc: 0.7670\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8573 - acc: 0.7863 - val_loss: 0.8811 - val_acc: 0.7800\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8580 - acc: 0.7875 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8581 - acc: 0.7884 - val_loss: 0.8864 - val_acc: 0.7630\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8568 - acc: 0.7860 - val_loss: 0.8818 - val_acc: 0.7700\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8582 - acc: 0.7865 - val_loss: 0.8820 - val_acc: 0.7780\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8578 - acc: 0.7875 - val_loss: 0.8899 - val_acc: 0.7640\n",
      "Epoch 708/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7865 - val_loss: 0.8828 - val_acc: 0.7630\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8559 - acc: 0.7872 - val_loss: 0.8801 - val_acc: 0.7730\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8576 - acc: 0.7848 - val_loss: 0.9000 - val_acc: 0.7660\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8566 - acc: 0.7859 - val_loss: 0.8860 - val_acc: 0.7610\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8575 - acc: 0.7876 - val_loss: 0.8777 - val_acc: 0.7790\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8551 - acc: 0.7885 - val_loss: 0.8820 - val_acc: 0.7680\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8577 - acc: 0.7881 - val_loss: 0.8791 - val_acc: 0.7750\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7861 - val_loss: 0.8865 - val_acc: 0.7650\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8570 - acc: 0.7872 - val_loss: 0.8798 - val_acc: 0.7790\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8555 - acc: 0.7875 - val_loss: 0.8794 - val_acc: 0.7690\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8556 - acc: 0.7888 - val_loss: 0.8802 - val_acc: 0.7760\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8554 - acc: 0.7879 - val_loss: 0.8878 - val_acc: 0.7720\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8542 - acc: 0.7892 - val_loss: 0.8827 - val_acc: 0.7630\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8553 - acc: 0.7869 - val_loss: 0.8850 - val_acc: 0.7740\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8555 - acc: 0.7856 - val_loss: 0.8881 - val_acc: 0.7750\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8531 - acc: 0.7888 - val_loss: 0.8868 - val_acc: 0.7720\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8547 - acc: 0.7872 - val_loss: 0.8804 - val_acc: 0.7740\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8552 - acc: 0.7867 - val_loss: 0.8799 - val_acc: 0.7660\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8550 - acc: 0.7875 - val_loss: 0.8773 - val_acc: 0.7790\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8541 - acc: 0.7867 - val_loss: 0.8778 - val_acc: 0.7790\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8556 - acc: 0.7880 - val_loss: 0.8768 - val_acc: 0.7740\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8545 - acc: 0.7876 - val_loss: 0.8845 - val_acc: 0.7570\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8534 - acc: 0.7879 - val_loss: 0.8850 - val_acc: 0.7700\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8546 - acc: 0.7881 - val_loss: 0.8927 - val_acc: 0.7640\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8547 - acc: 0.7888 - val_loss: 0.8765 - val_acc: 0.7770\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8542 - acc: 0.7877 - val_loss: 0.8846 - val_acc: 0.7620\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8529 - acc: 0.7884 - val_loss: 0.8797 - val_acc: 0.7690\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8535 - acc: 0.7871 - val_loss: 0.8825 - val_acc: 0.7690\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7883 - val_loss: 0.9048 - val_acc: 0.7520\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7857 - val_loss: 0.8794 - val_acc: 0.7720\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8543 - acc: 0.7877 - val_loss: 0.8813 - val_acc: 0.7730\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8521 - acc: 0.7877 - val_loss: 0.8790 - val_acc: 0.7610\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7790\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7885 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7875 - val_loss: 0.8800 - val_acc: 0.7770\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7863 - val_loss: 0.8798 - val_acc: 0.7620\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7883 - val_loss: 0.8860 - val_acc: 0.7620\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8534 - acc: 0.7867 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7897 - val_loss: 0.8847 - val_acc: 0.7670\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8513 - acc: 0.7877 - val_loss: 0.8908 - val_acc: 0.7630\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8533 - acc: 0.7873 - val_loss: 0.8952 - val_acc: 0.7640\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8515 - acc: 0.7877 - val_loss: 0.9081 - val_acc: 0.7580\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8524 - acc: 0.7869 - val_loss: 0.8971 - val_acc: 0.7570\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8524 - acc: 0.7879 - val_loss: 0.8761 - val_acc: 0.7690\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8807 - val_acc: 0.7690\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8515 - acc: 0.7876 - val_loss: 0.8847 - val_acc: 0.7680\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8525 - acc: 0.7901 - val_loss: 0.8848 - val_acc: 0.7630\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8516 - acc: 0.7891 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8771 - val_acc: 0.7740\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8502 - acc: 0.7880 - val_loss: 0.8824 - val_acc: 0.7660\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7897 - val_loss: 0.8898 - val_acc: 0.7520\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8512 - acc: 0.7885 - val_loss: 0.8823 - val_acc: 0.7660\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8509 - acc: 0.7871 - val_loss: 0.8852 - val_acc: 0.7650\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8866 - val_acc: 0.7630\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7880 - val_loss: 0.9056 - val_acc: 0.7660\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8526 - acc: 0.7872 - val_loss: 0.8780 - val_acc: 0.7710\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8492 - acc: 0.7908 - val_loss: 0.9256 - val_acc: 0.7600\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8505 - acc: 0.7871 - val_loss: 0.8952 - val_acc: 0.7630\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8510 - acc: 0.7888 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8502 - acc: 0.7888 - val_loss: 0.8754 - val_acc: 0.7660\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8494 - acc: 0.7873 - val_loss: 0.8806 - val_acc: 0.7650\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8499 - acc: 0.7888 - val_loss: 0.8784 - val_acc: 0.7610\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8493 - acc: 0.7896 - val_loss: 0.8749 - val_acc: 0.7730\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8489 - acc: 0.7883 - val_loss: 0.8865 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8503 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7730\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8514 - acc: 0.7860 - val_loss: 0.8727 - val_acc: 0.7780\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8473 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7810\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8500 - acc: 0.7881 - val_loss: 0.8779 - val_acc: 0.7670\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8944 - val_acc: 0.7730\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8512 - acc: 0.7875 - val_loss: 0.8777 - val_acc: 0.7730\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8491 - acc: 0.7883 - val_loss: 0.8754 - val_acc: 0.7670\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8485 - acc: 0.7908 - val_loss: 0.8877 - val_acc: 0.7600\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8483 - acc: 0.7869 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8484 - acc: 0.7879 - val_loss: 0.8857 - val_acc: 0.7590\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8477 - acc: 0.7885 - val_loss: 0.8737 - val_acc: 0.7710\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7881 - val_loss: 0.8722 - val_acc: 0.7660\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7900 - val_loss: 0.8985 - val_acc: 0.7710\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8482 - acc: 0.7865 - val_loss: 0.8861 - val_acc: 0.7640\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8490 - acc: 0.7897 - val_loss: 0.8863 - val_acc: 0.7690\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8481 - acc: 0.7872 - val_loss: 0.8863 - val_acc: 0.7790\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8472 - acc: 0.7884 - val_loss: 0.8747 - val_acc: 0.7760\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.7921 - val_loss: 0.8806 - val_acc: 0.7730\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8463 - acc: 0.7892 - val_loss: 0.9155 - val_acc: 0.7620\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8494 - acc: 0.7888 - val_loss: 0.8986 - val_acc: 0.7580\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8471 - acc: 0.7880 - val_loss: 0.8724 - val_acc: 0.7720\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8464 - acc: 0.7911 - val_loss: 0.8766 - val_acc: 0.7700\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8464 - acc: 0.7901 - val_loss: 0.8803 - val_acc: 0.7680\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7919 - val_loss: 0.8756 - val_acc: 0.7760\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8457 - acc: 0.7896 - val_loss: 0.8704 - val_acc: 0.7780\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8467 - acc: 0.7897 - val_loss: 0.8741 - val_acc: 0.7680\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8451 - acc: 0.7889 - val_loss: 0.8812 - val_acc: 0.7730\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8475 - acc: 0.7888 - val_loss: 0.8841 - val_acc: 0.7680\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8459 - acc: 0.7901 - val_loss: 0.8758 - val_acc: 0.7640\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8454 - acc: 0.7911 - val_loss: 0.8855 - val_acc: 0.7680\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8478 - acc: 0.7895 - val_loss: 0.8714 - val_acc: 0.7760\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8447 - acc: 0.7931 - val_loss: 0.8870 - val_acc: 0.7630\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7917 - val_loss: 0.8808 - val_acc: 0.7760\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8469 - acc: 0.7899 - val_loss: 0.8759 - val_acc: 0.7670\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8457 - acc: 0.7880 - val_loss: 0.8780 - val_acc: 0.7730\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8454 - acc: 0.7921 - val_loss: 0.8760 - val_acc: 0.7710\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8456 - acc: 0.7892 - val_loss: 0.8776 - val_acc: 0.7700\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8449 - acc: 0.7893 - val_loss: 0.8746 - val_acc: 0.7680\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8450 - acc: 0.7915 - val_loss: 0.8837 - val_acc: 0.7720\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8455 - acc: 0.7911 - val_loss: 0.8805 - val_acc: 0.7600\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7899 - val_loss: 0.8708 - val_acc: 0.7720\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8440 - acc: 0.7892 - val_loss: 0.8731 - val_acc: 0.7820\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8433 - acc: 0.7921 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8447 - acc: 0.7893 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8438 - acc: 0.7905 - val_loss: 0.8796 - val_acc: 0.7650\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8442 - acc: 0.7887 - val_loss: 0.8811 - val_acc: 0.7710\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8439 - acc: 0.7887 - val_loss: 0.8814 - val_acc: 0.7660\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8444 - acc: 0.7895 - val_loss: 0.9006 - val_acc: 0.7600\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7895 - val_loss: 0.8831 - val_acc: 0.7550\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8427 - acc: 0.7885 - val_loss: 0.8728 - val_acc: 0.7700\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8431 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7760\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7899 - val_loss: 0.8770 - val_acc: 0.7700\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8447 - acc: 0.7880 - val_loss: 0.8745 - val_acc: 0.7650\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8428 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7760\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8425 - acc: 0.7915 - val_loss: 0.8753 - val_acc: 0.7740\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8437 - acc: 0.7893 - val_loss: 0.8756 - val_acc: 0.7630\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8419 - acc: 0.7913 - val_loss: 0.8763 - val_acc: 0.7740\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8435 - acc: 0.7900 - val_loss: 0.8850 - val_acc: 0.7780\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8432 - acc: 0.7911 - val_loss: 0.8700 - val_acc: 0.7760\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8421 - acc: 0.7883 - val_loss: 0.8707 - val_acc: 0.7790\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8446 - acc: 0.7892 - val_loss: 0.8699 - val_acc: 0.7750\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8420 - acc: 0.7919 - val_loss: 0.8727 - val_acc: 0.7700\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8420 - acc: 0.7923 - val_loss: 0.8736 - val_acc: 0.7640\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8415 - acc: 0.7896 - val_loss: 0.8838 - val_acc: 0.7550\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8415 - acc: 0.7917 - val_loss: 0.8744 - val_acc: 0.7670\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7892 - val_loss: 0.8892 - val_acc: 0.7550\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8431 - acc: 0.7903 - val_loss: 0.8729 - val_acc: 0.7710\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8410 - acc: 0.7931 - val_loss: 0.8717 - val_acc: 0.7700\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8413 - acc: 0.7903 - val_loss: 0.8742 - val_acc: 0.7740\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7908 - val_loss: 0.8823 - val_acc: 0.7750\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8423 - acc: 0.7908 - val_loss: 0.8700 - val_acc: 0.7700\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7907 - val_loss: 0.8698 - val_acc: 0.7720\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8405 - acc: 0.7913 - val_loss: 0.8691 - val_acc: 0.7740\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8429 - acc: 0.7896 - val_loss: 0.8698 - val_acc: 0.7740\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7912 - val_loss: 0.8722 - val_acc: 0.7700\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8410 - acc: 0.7901 - val_loss: 0.9618 - val_acc: 0.7380\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7873 - val_loss: 0.8821 - val_acc: 0.7790\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7900 - val_loss: 0.8696 - val_acc: 0.7820\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8402 - acc: 0.7912 - val_loss: 0.8697 - val_acc: 0.7610\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8392 - acc: 0.7917 - val_loss: 0.8730 - val_acc: 0.7640\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8392 - acc: 0.7928 - val_loss: 0.8805 - val_acc: 0.7570\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8410 - acc: 0.7911 - val_loss: 0.8692 - val_acc: 0.7690\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8393 - acc: 0.7912 - val_loss: 0.8701 - val_acc: 0.7720\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8404 - acc: 0.7904 - val_loss: 0.8670 - val_acc: 0.7790\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7921 - val_loss: 0.8851 - val_acc: 0.7690\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8400 - acc: 0.7905 - val_loss: 0.8999 - val_acc: 0.7610\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8409 - acc: 0.7907 - val_loss: 0.8827 - val_acc: 0.7700\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8418 - acc: 0.7880 - val_loss: 0.8833 - val_acc: 0.7710\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8380 - acc: 0.7939 - val_loss: 0.8869 - val_acc: 0.7600\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8395 - acc: 0.7927 - val_loss: 0.8714 - val_acc: 0.7600\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8386 - acc: 0.7903 - val_loss: 0.8788 - val_acc: 0.7680\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8409 - acc: 0.7909 - val_loss: 0.8762 - val_acc: 0.7700\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7908 - val_loss: 0.8687 - val_acc: 0.7720\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7929 - val_loss: 0.8948 - val_acc: 0.7550\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7925 - val_loss: 0.8860 - val_acc: 0.7680\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8376 - acc: 0.7923 - val_loss: 0.8703 - val_acc: 0.7740\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8389 - acc: 0.7901 - val_loss: 0.8712 - val_acc: 0.7650\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7921 - val_loss: 0.8751 - val_acc: 0.7640\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8391 - acc: 0.7933 - val_loss: 0.8676 - val_acc: 0.7740\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7920 - val_loss: 0.8739 - val_acc: 0.7710\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8379 - acc: 0.7924 - val_loss: 0.8740 - val_acc: 0.7750\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8370 - acc: 0.7909 - val_loss: 0.8694 - val_acc: 0.7780\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7924 - val_loss: 0.8706 - val_acc: 0.7780\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8360 - acc: 0.7919 - val_loss: 0.8774 - val_acc: 0.7750\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8364 - acc: 0.7953 - val_loss: 0.8750 - val_acc: 0.7700\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8387 - acc: 0.7908 - val_loss: 0.8785 - val_acc: 0.7660\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8378 - acc: 0.7920 - val_loss: 0.8692 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8357 - acc: 0.7912 - val_loss: 0.8671 - val_acc: 0.7770\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8378 - acc: 0.7921 - val_loss: 0.8758 - val_acc: 0.7720\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8380 - acc: 0.7924 - val_loss: 0.8648 - val_acc: 0.7760\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8372 - acc: 0.7929 - val_loss: 0.8806 - val_acc: 0.7570\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8369 - acc: 0.7915 - val_loss: 0.8671 - val_acc: 0.7760\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8381 - acc: 0.7924 - val_loss: 0.8780 - val_acc: 0.7590\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8364 - acc: 0.7929 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8360 - acc: 0.7935 - val_loss: 0.8719 - val_acc: 0.7640\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8375 - acc: 0.7937 - val_loss: 0.8741 - val_acc: 0.7720\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8369 - acc: 0.7924 - val_loss: 0.8723 - val_acc: 0.7700\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8362 - acc: 0.7937 - val_loss: 0.8802 - val_acc: 0.7640\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8361 - acc: 0.7883 - val_loss: 0.8674 - val_acc: 0.7750\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8349 - acc: 0.7940 - val_loss: 0.8714 - val_acc: 0.7690\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8356 - acc: 0.7939 - val_loss: 0.8732 - val_acc: 0.7630\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8354 - acc: 0.7935 - val_loss: 0.8716 - val_acc: 0.7700\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8361 - acc: 0.7916 - val_loss: 0.8665 - val_acc: 0.7740\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8353 - acc: 0.7949 - val_loss: 0.8713 - val_acc: 0.7790\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8746 - val_acc: 0.7700\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8351 - acc: 0.7912 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8370 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7730\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7931 - val_loss: 0.8860 - val_acc: 0.7580\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8353 - acc: 0.7915 - val_loss: 0.8719 - val_acc: 0.7750\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8356 - acc: 0.7935 - val_loss: 0.8811 - val_acc: 0.7600\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8350 - acc: 0.7932 - val_loss: 0.8670 - val_acc: 0.7700\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8361 - acc: 0.7917 - val_loss: 0.8775 - val_acc: 0.7670\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8366 - acc: 0.7927 - val_loss: 0.8785 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8349 - acc: 0.7931 - val_loss: 0.8660 - val_acc: 0.7820\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8348 - acc: 0.7925 - val_loss: 0.8749 - val_acc: 0.7630\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8355 - acc: 0.7924 - val_loss: 0.8714 - val_acc: 0.7740\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8357 - acc: 0.7931 - val_loss: 0.8642 - val_acc: 0.7800\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8336 - acc: 0.7924 - val_loss: 0.8703 - val_acc: 0.7630\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8331 - acc: 0.7953 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8329 - acc: 0.7939 - val_loss: 0.8785 - val_acc: 0.7590\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8721 - val_acc: 0.7800\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8345 - acc: 0.7949 - val_loss: 0.9010 - val_acc: 0.7670\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7913 - val_loss: 0.8663 - val_acc: 0.7790\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8332 - acc: 0.7947 - val_loss: 0.8676 - val_acc: 0.7700\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8328 - acc: 0.7931 - val_loss: 0.8873 - val_acc: 0.7640\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8343 - acc: 0.7940 - val_loss: 0.8788 - val_acc: 0.7690\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8333 - acc: 0.7933 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8334 - acc: 0.7937 - val_loss: 0.8792 - val_acc: 0.7650\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7905 - val_loss: 0.8708 - val_acc: 0.7700\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8318 - acc: 0.7947 - val_loss: 0.8702 - val_acc: 0.7780\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8331 - acc: 0.7961 - val_loss: 0.8727 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7940 - val_loss: 0.8657 - val_acc: 0.7670\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8320 - acc: 0.7917 - val_loss: 0.8748 - val_acc: 0.7620\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8348 - acc: 0.7956 - val_loss: 0.8673 - val_acc: 0.7710\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8332 - acc: 0.7928 - val_loss: 0.8731 - val_acc: 0.7700\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8332 - acc: 0.7948 - val_loss: 0.8649 - val_acc: 0.7750\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8340 - acc: 0.7935 - val_loss: 0.8692 - val_acc: 0.7730\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8743 - val_acc: 0.7750\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8353 - acc: 0.7935 - val_loss: 0.8815 - val_acc: 0.7590\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8325 - acc: 0.7924 - val_loss: 0.8768 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8309 - acc: 0.7928 - val_loss: 0.8755 - val_acc: 0.7800\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8327 - acc: 0.7940 - val_loss: 0.8750 - val_acc: 0.7760\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7943 - val_loss: 0.8758 - val_acc: 0.7630\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8316 - acc: 0.7937 - val_loss: 0.8810 - val_acc: 0.7650\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8310 - acc: 0.7932 - val_loss: 0.8650 - val_acc: 0.7790\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8313 - acc: 0.7931 - val_loss: 0.8739 - val_acc: 0.7790\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8301 - acc: 0.7935 - val_loss: 0.8809 - val_acc: 0.7660\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8326 - acc: 0.7952 - val_loss: 0.8729 - val_acc: 0.7660\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7915 - val_loss: 0.8750 - val_acc: 0.7770\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8314 - acc: 0.7937 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8322 - acc: 0.7944 - val_loss: 0.8653 - val_acc: 0.7800\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8303 - acc: 0.7969 - val_loss: 0.8620 - val_acc: 0.7780\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8307 - acc: 0.7940 - val_loss: 0.9117 - val_acc: 0.7530\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8722 - val_acc: 0.7640\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8318 - acc: 0.7956 - val_loss: 0.9060 - val_acc: 0.7530\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7940 - val_loss: 0.8882 - val_acc: 0.7560\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7953 - val_loss: 0.8652 - val_acc: 0.7710\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8320 - acc: 0.7960 - val_loss: 0.8838 - val_acc: 0.7720\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7943 - val_loss: 0.8648 - val_acc: 0.7820\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8288 - acc: 0.7957 - val_loss: 0.8646 - val_acc: 0.7750\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8293 - acc: 0.7955 - val_loss: 0.8815 - val_acc: 0.7700\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8303 - acc: 0.7967 - val_loss: 0.8624 - val_acc: 0.7730\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8296 - acc: 0.7955 - val_loss: 0.8666 - val_acc: 0.7700\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8304 - acc: 0.7936 - val_loss: 0.8820 - val_acc: 0.7740\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8294 - acc: 0.7951 - val_loss: 0.8776 - val_acc: 0.7760\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8311 - acc: 0.7921 - val_loss: 0.9091 - val_acc: 0.7580\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8301 - acc: 0.7952 - val_loss: 0.8609 - val_acc: 0.7770\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7951 - val_loss: 0.8673 - val_acc: 0.7700\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8315 - acc: 0.7940 - val_loss: 0.8695 - val_acc: 0.7740\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8308 - acc: 0.7948 - val_loss: 0.8662 - val_acc: 0.7700\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.7959 - val_loss: 0.8629 - val_acc: 0.7800\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8621 - val_acc: 0.7790\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8309 - acc: 0.7948 - val_loss: 0.8687 - val_acc: 0.7730\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8297 - acc: 0.7956 - val_loss: 0.8656 - val_acc: 0.7730\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8275 - acc: 0.7956 - val_loss: 0.8643 - val_acc: 0.7700\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7937 - val_loss: 0.8645 - val_acc: 0.7780\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8661 - val_acc: 0.7810\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7937 - val_loss: 0.8648 - val_acc: 0.7770\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8291 - acc: 0.7952 - val_loss: 0.8656 - val_acc: 0.7810\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7952 - val_loss: 0.8651 - val_acc: 0.7770\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8281 - acc: 0.7944 - val_loss: 0.8798 - val_acc: 0.7690\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8282 - acc: 0.7987 - val_loss: 0.8627 - val_acc: 0.7750\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8275 - acc: 0.7961 - val_loss: 0.8654 - val_acc: 0.7710\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7956 - val_loss: 0.8720 - val_acc: 0.7670\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8328 - acc: 0.7943 - val_loss: 0.9056 - val_acc: 0.7610\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7955 - val_loss: 0.8746 - val_acc: 0.7730\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8295 - acc: 0.7947 - val_loss: 0.8683 - val_acc: 0.7670\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8262 - acc: 0.7971 - val_loss: 0.8672 - val_acc: 0.7750\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8269 - acc: 0.7943 - val_loss: 0.8624 - val_acc: 0.7740\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8274 - acc: 0.7972 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8274 - acc: 0.7949 - val_loss: 0.8687 - val_acc: 0.7670\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8260 - acc: 0.7971 - val_loss: 0.9091 - val_acc: 0.7640\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8273 - acc: 0.7960 - val_loss: 0.8981 - val_acc: 0.7720\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8323 - acc: 0.7923 - val_loss: 0.8765 - val_acc: 0.7760\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8276 - acc: 0.7931 - val_loss: 0.8668 - val_acc: 0.7680\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7952 - val_loss: 0.8701 - val_acc: 0.7690\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8278 - acc: 0.7963 - val_loss: 0.8693 - val_acc: 0.7620\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7953 - val_loss: 0.8649 - val_acc: 0.7690\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7981 - val_loss: 0.8839 - val_acc: 0.7730\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8277 - acc: 0.7940 - val_loss: 0.8716 - val_acc: 0.7770\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8268 - acc: 0.7971 - val_loss: 0.8685 - val_acc: 0.7830\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.7952 - val_loss: 0.8832 - val_acc: 0.7540\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7960 - val_loss: 0.8651 - val_acc: 0.7650\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8261 - acc: 0.7964 - val_loss: 0.8932 - val_acc: 0.7690\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8298 - acc: 0.7944 - val_loss: 0.8595 - val_acc: 0.7830\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8243 - acc: 0.7968 - val_loss: 0.8693 - val_acc: 0.7730\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8247 - acc: 0.7968 - val_loss: 0.8596 - val_acc: 0.7790\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.7988 - val_loss: 0.8680 - val_acc: 0.7660\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8262 - acc: 0.7965 - val_loss: 0.8655 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8237653533299764, 0.7967999999682108]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.966706668694814, 0.7499999998410543]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.9778 - acc: 0.1645 - val_loss: 1.9369 - val_acc: 0.1910\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.9505 - acc: 0.1728 - val_loss: 1.9281 - val_acc: 0.2050\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.9424 - acc: 0.1837 - val_loss: 1.9213 - val_acc: 0.2200\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.9319 - acc: 0.1952 - val_loss: 1.9154 - val_acc: 0.2200\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.9222 - acc: 0.2020 - val_loss: 1.9092 - val_acc: 0.2220\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.9139 - acc: 0.2051 - val_loss: 1.9027 - val_acc: 0.2290\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.9117 - acc: 0.2132 - val_loss: 1.8957 - val_acc: 0.2380\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.9014 - acc: 0.2232 - val_loss: 1.8881 - val_acc: 0.2380\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8959 - acc: 0.2280 - val_loss: 1.8790 - val_acc: 0.2480\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.8879 - acc: 0.2325 - val_loss: 1.8680 - val_acc: 0.2710\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.8732 - acc: 0.2444 - val_loss: 1.8552 - val_acc: 0.2860\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.8747 - acc: 0.2387 - val_loss: 1.8447 - val_acc: 0.2960\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8571 - acc: 0.2576 - val_loss: 1.8310 - val_acc: 0.3030\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.8491 - acc: 0.2569 - val_loss: 1.8162 - val_acc: 0.3190\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8416 - acc: 0.2589 - val_loss: 1.8009 - val_acc: 0.3310\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8254 - acc: 0.2755 - val_loss: 1.7826 - val_acc: 0.3450\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 1.8149 - acc: 0.2872 - val_loss: 1.7653 - val_acc: 0.3570\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.7928 - acc: 0.2949 - val_loss: 1.7463 - val_acc: 0.3790\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.7804 - acc: 0.3015 - val_loss: 1.7249 - val_acc: 0.3930\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.7740 - acc: 0.3029 - val_loss: 1.7065 - val_acc: 0.4050\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.7564 - acc: 0.3149 - val_loss: 1.6851 - val_acc: 0.4190\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.7400 - acc: 0.3208 - val_loss: 1.6635 - val_acc: 0.4200\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.7182 - acc: 0.3351 - val_loss: 1.6409 - val_acc: 0.4350\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.7064 - acc: 0.3444 - val_loss: 1.6196 - val_acc: 0.4480\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6949 - acc: 0.3379 - val_loss: 1.6003 - val_acc: 0.4570\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6802 - acc: 0.3504 - val_loss: 1.5812 - val_acc: 0.4730\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6601 - acc: 0.3568 - val_loss: 1.5578 - val_acc: 0.4890\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.6468 - acc: 0.3668 - val_loss: 1.5362 - val_acc: 0.4920\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.6440 - acc: 0.3685 - val_loss: 1.5164 - val_acc: 0.5080\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.6181 - acc: 0.3821 - val_loss: 1.4961 - val_acc: 0.5230\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6118 - acc: 0.3801 - val_loss: 1.4772 - val_acc: 0.5370\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.5872 - acc: 0.3928 - val_loss: 1.4566 - val_acc: 0.5450\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.5842 - acc: 0.3964 - val_loss: 1.4400 - val_acc: 0.5600\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.5570 - acc: 0.4137 - val_loss: 1.4187 - val_acc: 0.5690\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5489 - acc: 0.4016 - val_loss: 1.4021 - val_acc: 0.5730\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.5270 - acc: 0.4189 - val_loss: 1.3835 - val_acc: 0.5740\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5207 - acc: 0.4201 - val_loss: 1.3640 - val_acc: 0.5870\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.5028 - acc: 0.4243 - val_loss: 1.3445 - val_acc: 0.5970\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.4907 - acc: 0.4277 - val_loss: 1.3256 - val_acc: 0.6090\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.4806 - acc: 0.4380 - val_loss: 1.3070 - val_acc: 0.6200\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.4745 - acc: 0.4348 - val_loss: 1.2920 - val_acc: 0.6240\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.4540 - acc: 0.4457 - val_loss: 1.2751 - val_acc: 0.6280\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.4451 - acc: 0.4493 - val_loss: 1.2604 - val_acc: 0.6290\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.4343 - acc: 0.4536 - val_loss: 1.2464 - val_acc: 0.6370\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.4162 - acc: 0.4656 - val_loss: 1.2273 - val_acc: 0.6340\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.4051 - acc: 0.4624 - val_loss: 1.2117 - val_acc: 0.6460\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.4006 - acc: 0.4657 - val_loss: 1.1975 - val_acc: 0.6490\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.3866 - acc: 0.4795 - val_loss: 1.1860 - val_acc: 0.6440\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.3632 - acc: 0.4808 - val_loss: 1.1665 - val_acc: 0.6570\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.3622 - acc: 0.4807 - val_loss: 1.1561 - val_acc: 0.6560\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.3467 - acc: 0.4871 - val_loss: 1.1429 - val_acc: 0.6610\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.3420 - acc: 0.4899 - val_loss: 1.1291 - val_acc: 0.6690\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.3339 - acc: 0.4904 - val_loss: 1.1175 - val_acc: 0.6690\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3277 - acc: 0.4963 - val_loss: 1.1080 - val_acc: 0.6730\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3269 - acc: 0.4960 - val_loss: 1.0979 - val_acc: 0.6820\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2926 - acc: 0.5140 - val_loss: 1.0812 - val_acc: 0.6840\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.2878 - acc: 0.5121 - val_loss: 1.0678 - val_acc: 0.6870\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2860 - acc: 0.5103 - val_loss: 1.0570 - val_acc: 0.6900\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2849 - acc: 0.5117 - val_loss: 1.0461 - val_acc: 0.6900\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2701 - acc: 0.5280 - val_loss: 1.0348 - val_acc: 0.6880\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 1.2468 - acc: 0.5265 - val_loss: 1.0223 - val_acc: 0.6890\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.2470 - acc: 0.5267 - val_loss: 1.0130 - val_acc: 0.6960\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.2433 - acc: 0.5271 - val_loss: 1.0054 - val_acc: 0.7000\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2233 - acc: 0.5383 - val_loss: 0.9938 - val_acc: 0.7040\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2240 - acc: 0.5363 - val_loss: 0.9842 - val_acc: 0.7040\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.2066 - acc: 0.5408 - val_loss: 0.9734 - val_acc: 0.7030\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.2026 - acc: 0.5477 - val_loss: 0.9662 - val_acc: 0.7030\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.1997 - acc: 0.5487 - val_loss: 0.9582 - val_acc: 0.7060\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1948 - acc: 0.5480 - val_loss: 0.9520 - val_acc: 0.7080\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.1824 - acc: 0.5627 - val_loss: 0.9417 - val_acc: 0.7090\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.1847 - acc: 0.5552 - val_loss: 0.9378 - val_acc: 0.7070\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.1711 - acc: 0.5572 - val_loss: 0.9275 - val_acc: 0.7210\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.1683 - acc: 0.5653 - val_loss: 0.9210 - val_acc: 0.7130\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1536 - acc: 0.5711 - val_loss: 0.9101 - val_acc: 0.7230\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.1683 - acc: 0.5571 - val_loss: 0.9082 - val_acc: 0.7200\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.1483 - acc: 0.5633 - val_loss: 0.8986 - val_acc: 0.7220\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.1404 - acc: 0.5740 - val_loss: 0.8929 - val_acc: 0.7290\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.1407 - acc: 0.5767 - val_loss: 0.8862 - val_acc: 0.7220\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1178 - acc: 0.5820 - val_loss: 0.8773 - val_acc: 0.7210\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 1.1273 - acc: 0.5788 - val_loss: 0.8721 - val_acc: 0.7310\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.1193 - acc: 0.5792 - val_loss: 0.8661 - val_acc: 0.7300\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1064 - acc: 0.5817 - val_loss: 0.8604 - val_acc: 0.7280\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.1088 - acc: 0.5912 - val_loss: 0.8519 - val_acc: 0.7270\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0926 - acc: 0.5981 - val_loss: 0.8467 - val_acc: 0.7360\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0977 - acc: 0.5884 - val_loss: 0.8410 - val_acc: 0.7370\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0836 - acc: 0.5939 - val_loss: 0.8338 - val_acc: 0.7370\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.0877 - acc: 0.594 - 1s 74us/step - loss: 1.0843 - acc: 0.5943 - val_loss: 0.8289 - val_acc: 0.7460\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0714 - acc: 0.5989 - val_loss: 0.8242 - val_acc: 0.7390\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.0767 - acc: 0.5893 - val_loss: 0.8217 - val_acc: 0.7390\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.0687 - acc: 0.6005 - val_loss: 0.8145 - val_acc: 0.7430\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.0617 - acc: 0.6012 - val_loss: 0.8088 - val_acc: 0.7440\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 1.0595 - acc: 0.5987 - val_loss: 0.8093 - val_acc: 0.7420\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 1.0444 - acc: 0.6143 - val_loss: 0.8046 - val_acc: 0.7470\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.0418 - acc: 0.6149 - val_loss: 0.7953 - val_acc: 0.7460\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.0321 - acc: 0.6181 - val_loss: 0.7921 - val_acc: 0.7480\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0398 - acc: 0.6121 - val_loss: 0.7884 - val_acc: 0.7460\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0450 - acc: 0.6084 - val_loss: 0.7836 - val_acc: 0.7510\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.0317 - acc: 0.6153 - val_loss: 0.7783 - val_acc: 0.7490\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.0256 - acc: 0.6192 - val_loss: 0.7738 - val_acc: 0.7570\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.0106 - acc: 0.6256 - val_loss: 0.7685 - val_acc: 0.7570\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.0202 - acc: 0.6213 - val_loss: 0.7670 - val_acc: 0.7520\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0085 - acc: 0.6301 - val_loss: 0.7644 - val_acc: 0.7550\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0134 - acc: 0.6172 - val_loss: 0.7610 - val_acc: 0.7540\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9989 - acc: 0.6264 - val_loss: 0.7571 - val_acc: 0.7510\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.0009 - acc: 0.6344 - val_loss: 0.7511 - val_acc: 0.7600\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0018 - acc: 0.6288 - val_loss: 0.7469 - val_acc: 0.7590\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.0049 - acc: 0.6247 - val_loss: 0.7475 - val_acc: 0.7590\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9896 - acc: 0.6280 - val_loss: 0.7454 - val_acc: 0.7590\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9854 - acc: 0.6431 - val_loss: 0.7410 - val_acc: 0.7620\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9865 - acc: 0.6315 - val_loss: 0.7368 - val_acc: 0.7590\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.9675 - acc: 0.6400 - val_loss: 0.7350 - val_acc: 0.7610\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9638 - acc: 0.6364 - val_loss: 0.7297 - val_acc: 0.7630\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9696 - acc: 0.6400 - val_loss: 0.7289 - val_acc: 0.7610\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.9656 - acc: 0.6407 - val_loss: 0.7270 - val_acc: 0.7610\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9703 - acc: 0.6423 - val_loss: 0.7211 - val_acc: 0.7650\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9598 - acc: 0.6365 - val_loss: 0.7187 - val_acc: 0.7680\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9513 - acc: 0.6461 - val_loss: 0.7168 - val_acc: 0.7630\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9648 - acc: 0.6419 - val_loss: 0.7150 - val_acc: 0.7630\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9540 - acc: 0.6493 - val_loss: 0.7117 - val_acc: 0.7660\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9459 - acc: 0.6500 - val_loss: 0.7085 - val_acc: 0.7640\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.9498 - acc: 0.6484 - val_loss: 0.7058 - val_acc: 0.7650\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9465 - acc: 0.6517 - val_loss: 0.7054 - val_acc: 0.7640\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9362 - acc: 0.6523 - val_loss: 0.7027 - val_acc: 0.7630\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9432 - acc: 0.6495 - val_loss: 0.7008 - val_acc: 0.7680\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9360 - acc: 0.6608 - val_loss: 0.6992 - val_acc: 0.7720\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9191 - acc: 0.6641 - val_loss: 0.6931 - val_acc: 0.7690\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9298 - acc: 0.6507 - val_loss: 0.6911 - val_acc: 0.7690\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9307 - acc: 0.6591 - val_loss: 0.6930 - val_acc: 0.7680\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.9115 - acc: 0.6624 - val_loss: 0.6910 - val_acc: 0.7710\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9082 - acc: 0.6633 - val_loss: 0.6868 - val_acc: 0.7670\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.9228 - acc: 0.6583 - val_loss: 0.6833 - val_acc: 0.7700\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9089 - acc: 0.6628 - val_loss: 0.6822 - val_acc: 0.7750\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9131 - acc: 0.6581 - val_loss: 0.6790 - val_acc: 0.7700\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9136 - acc: 0.6652 - val_loss: 0.6792 - val_acc: 0.7670\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9029 - acc: 0.6661 - val_loss: 0.6765 - val_acc: 0.7700\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9053 - acc: 0.6637 - val_loss: 0.6758 - val_acc: 0.7710\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8883 - acc: 0.6692 - val_loss: 0.6751 - val_acc: 0.7710\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.9032 - acc: 0.6676 - val_loss: 0.6725 - val_acc: 0.7710\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8899 - acc: 0.6759 - val_loss: 0.6710 - val_acc: 0.7740\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8932 - acc: 0.6687 - val_loss: 0.6686 - val_acc: 0.7690\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8827 - acc: 0.6728 - val_loss: 0.6660 - val_acc: 0.7760\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8855 - acc: 0.6777 - val_loss: 0.6670 - val_acc: 0.7690\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8696 - acc: 0.6781 - val_loss: 0.6614 - val_acc: 0.7720\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8780 - acc: 0.6736 - val_loss: 0.6635 - val_acc: 0.7730\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8672 - acc: 0.6764 - val_loss: 0.6575 - val_acc: 0.7760\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8718 - acc: 0.6755 - val_loss: 0.6581 - val_acc: 0.7740\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8715 - acc: 0.6712 - val_loss: 0.6569 - val_acc: 0.7740\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8628 - acc: 0.6793 - val_loss: 0.6543 - val_acc: 0.7760\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8714 - acc: 0.6708 - val_loss: 0.6534 - val_acc: 0.7700\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8675 - acc: 0.6735 - val_loss: 0.6535 - val_acc: 0.7740\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8573 - acc: 0.6808 - val_loss: 0.6538 - val_acc: 0.7710\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8544 - acc: 0.6796 - val_loss: 0.6491 - val_acc: 0.7720\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8550 - acc: 0.6853 - val_loss: 0.6487 - val_acc: 0.7710\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8384 - acc: 0.6877 - val_loss: 0.6420 - val_acc: 0.7750\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8382 - acc: 0.6851 - val_loss: 0.6431 - val_acc: 0.7700\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8525 - acc: 0.6791 - val_loss: 0.6405 - val_acc: 0.7780\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8435 - acc: 0.6925 - val_loss: 0.6413 - val_acc: 0.7720\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8453 - acc: 0.6875 - val_loss: 0.6380 - val_acc: 0.7720\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8281 - acc: 0.6905 - val_loss: 0.6365 - val_acc: 0.7710\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8611 - acc: 0.6803 - val_loss: 0.6370 - val_acc: 0.7700\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8386 - acc: 0.6925 - val_loss: 0.6351 - val_acc: 0.7700\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8254 - acc: 0.6943 - val_loss: 0.6340 - val_acc: 0.7760\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8455 - acc: 0.6856 - val_loss: 0.6321 - val_acc: 0.7750\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8400 - acc: 0.6941 - val_loss: 0.6307 - val_acc: 0.7790\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8355 - acc: 0.6896 - val_loss: 0.6275 - val_acc: 0.7770\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8171 - acc: 0.7053 - val_loss: 0.6277 - val_acc: 0.7770\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8191 - acc: 0.6985 - val_loss: 0.6290 - val_acc: 0.7740\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8180 - acc: 0.6976 - val_loss: 0.6238 - val_acc: 0.7770\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8067 - acc: 0.6925 - val_loss: 0.6244 - val_acc: 0.7820\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8225 - acc: 0.6909 - val_loss: 0.6240 - val_acc: 0.7800\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8107 - acc: 0.6973 - val_loss: 0.6241 - val_acc: 0.7720\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8223 - acc: 0.6917 - val_loss: 0.6243 - val_acc: 0.7720\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8188 - acc: 0.6988 - val_loss: 0.6214 - val_acc: 0.7770\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8128 - acc: 0.7005 - val_loss: 0.6203 - val_acc: 0.7750\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8150 - acc: 0.6981 - val_loss: 0.6210 - val_acc: 0.7780\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8173 - acc: 0.6932 - val_loss: 0.6173 - val_acc: 0.7820\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8078 - acc: 0.6977 - val_loss: 0.6214 - val_acc: 0.7790\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8058 - acc: 0.7001 - val_loss: 0.6171 - val_acc: 0.7810\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8040 - acc: 0.7067 - val_loss: 0.6160 - val_acc: 0.7780\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7990 - acc: 0.7079 - val_loss: 0.6136 - val_acc: 0.7790\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7921 - acc: 0.6964 - val_loss: 0.6121 - val_acc: 0.7780\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7958 - acc: 0.7029 - val_loss: 0.6109 - val_acc: 0.7820\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8001 - acc: 0.7072 - val_loss: 0.6099 - val_acc: 0.7800\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7817 - acc: 0.7137 - val_loss: 0.6088 - val_acc: 0.7780\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8059 - acc: 0.6940 - val_loss: 0.6097 - val_acc: 0.7800\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7936 - acc: 0.7016 - val_loss: 0.6075 - val_acc: 0.7790\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7858 - acc: 0.7101 - val_loss: 0.6083 - val_acc: 0.7820\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7837 - acc: 0.7052 - val_loss: 0.6079 - val_acc: 0.7820\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7936 - acc: 0.7016 - val_loss: 0.6066 - val_acc: 0.7780\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7876 - acc: 0.7105 - val_loss: 0.6050 - val_acc: 0.7820\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7797 - acc: 0.7105 - val_loss: 0.6047 - val_acc: 0.7810\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7902 - acc: 0.7051 - val_loss: 0.6030 - val_acc: 0.7840\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7868 - acc: 0.7049 - val_loss: 0.6039 - val_acc: 0.7830\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7952 - acc: 0.7123 - val_loss: 0.6029 - val_acc: 0.7770\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7758 - acc: 0.7047 - val_loss: 0.5995 - val_acc: 0.7790\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7726 - acc: 0.7175 - val_loss: 0.5994 - val_acc: 0.7800\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7679 - acc: 0.7153 - val_loss: 0.6006 - val_acc: 0.7840\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7806 - acc: 0.7115 - val_loss: 0.5979 - val_acc: 0.7830\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7726 - acc: 0.7127 - val_loss: 0.5996 - val_acc: 0.7850\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7655 - acc: 0.7184 - val_loss: 0.5964 - val_acc: 0.7810\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step\n",
      "1500/1500 [==============================] - 0s 33us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47885879406929016, 0.8412]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6310157674948375, 0.749333333492279]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 1.8642 - acc: 0.2353 - val_loss: 1.7748 - val_acc: 0.3233\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 1.6778 - acc: 0.3796 - val_loss: 1.5501 - val_acc: 0.4710\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 1.4372 - acc: 0.5258 - val_loss: 1.3044 - val_acc: 0.6010\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 1.2136 - acc: 0.6187 - val_loss: 1.1072 - val_acc: 0.6613\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 1.0465 - acc: 0.6668 - val_loss: 0.9709 - val_acc: 0.6907\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.9313 - acc: 0.6921 - val_loss: 0.8776 - val_acc: 0.7067\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.8515 - acc: 0.7089 - val_loss: 0.8139 - val_acc: 0.7250\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.7948 - acc: 0.7222 - val_loss: 0.7646 - val_acc: 0.7337\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.7521 - acc: 0.7312 - val_loss: 0.7312 - val_acc: 0.7447\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.7192 - acc: 0.7408 - val_loss: 0.7033 - val_acc: 0.7457\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.6928 - acc: 0.7483 - val_loss: 0.6826 - val_acc: 0.7540\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.6710 - acc: 0.7528 - val_loss: 0.6649 - val_acc: 0.7587\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.6525 - acc: 0.7588 - val_loss: 0.6485 - val_acc: 0.7670\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.6365 - acc: 0.7662 - val_loss: 0.6379 - val_acc: 0.7727\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.6226 - acc: 0.7684 - val_loss: 0.6255 - val_acc: 0.7787\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.6102 - acc: 0.7741 - val_loss: 0.6165 - val_acc: 0.7787\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5990 - acc: 0.7774 - val_loss: 0.6080 - val_acc: 0.7787\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.5888 - acc: 0.7819 - val_loss: 0.6019 - val_acc: 0.7803\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.5792 - acc: 0.7850 - val_loss: 0.5960 - val_acc: 0.7853\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5708 - acc: 0.7878 - val_loss: 0.5895 - val_acc: 0.7910\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5627 - acc: 0.7914 - val_loss: 0.5853 - val_acc: 0.7850\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5548 - acc: 0.7944 - val_loss: 0.5806 - val_acc: 0.7850\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5475 - acc: 0.7979 - val_loss: 0.5753 - val_acc: 0.7893\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5406 - acc: 0.8004 - val_loss: 0.5745 - val_acc: 0.7900\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.5343 - acc: 0.8025 - val_loss: 0.5679 - val_acc: 0.7913\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5281 - acc: 0.8059 - val_loss: 0.5650 - val_acc: 0.7937\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5219 - acc: 0.8088 - val_loss: 0.5617 - val_acc: 0.7963\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5166 - acc: 0.8117 - val_loss: 0.5592 - val_acc: 0.7983\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5111 - acc: 0.8141 - val_loss: 0.5578 - val_acc: 0.8000\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5060 - acc: 0.8155 - val_loss: 0.5534 - val_acc: 0.7990\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5013 - acc: 0.8171 - val_loss: 0.5512 - val_acc: 0.8023\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4963 - acc: 0.8211 - val_loss: 0.5513 - val_acc: 0.8000\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4914 - acc: 0.8215 - val_loss: 0.5504 - val_acc: 0.7977\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4873 - acc: 0.8237 - val_loss: 0.5463 - val_acc: 0.8037\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4828 - acc: 0.8255 - val_loss: 0.5449 - val_acc: 0.8013\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4788 - acc: 0.8268 - val_loss: 0.5431 - val_acc: 0.8070\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4744 - acc: 0.8278 - val_loss: 0.5474 - val_acc: 0.8053\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4711 - acc: 0.8290 - val_loss: 0.5411 - val_acc: 0.8063\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4672 - acc: 0.8321 - val_loss: 0.5382 - val_acc: 0.8070\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4631 - acc: 0.8323 - val_loss: 0.5389 - val_acc: 0.8083\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4601 - acc: 0.8344 - val_loss: 0.5397 - val_acc: 0.8053\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4562 - acc: 0.8354 - val_loss: 0.5357 - val_acc: 0.8107\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4532 - acc: 0.8375 - val_loss: 0.5344 - val_acc: 0.8103\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4497 - acc: 0.8372 - val_loss: 0.5351 - val_acc: 0.8117\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4469 - acc: 0.8391 - val_loss: 0.5345 - val_acc: 0.8103\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4438 - acc: 0.8410 - val_loss: 0.5326 - val_acc: 0.8107\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4406 - acc: 0.8418 - val_loss: 0.5322 - val_acc: 0.8117\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4376 - acc: 0.8423 - val_loss: 0.5337 - val_acc: 0.8113\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4349 - acc: 0.8439 - val_loss: 0.5354 - val_acc: 0.8100\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4317 - acc: 0.8454 - val_loss: 0.5356 - val_acc: 0.8123\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4292 - acc: 0.8454 - val_loss: 0.5312 - val_acc: 0.8110\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4265 - acc: 0.8474 - val_loss: 0.5338 - val_acc: 0.8127\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4236 - acc: 0.8484 - val_loss: 0.5322 - val_acc: 0.8127\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4212 - acc: 0.8491 - val_loss: 0.5325 - val_acc: 0.8113\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4188 - acc: 0.8498 - val_loss: 0.5319 - val_acc: 0.8113\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4162 - acc: 0.8516 - val_loss: 0.5316 - val_acc: 0.8140\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.4138 - acc: 0.852 - 2s 49us/step - loss: 0.4138 - acc: 0.8518 - val_loss: 0.5296 - val_acc: 0.8160\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4118 - acc: 0.8539 - val_loss: 0.5371 - val_acc: 0.8113\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4092 - acc: 0.8543 - val_loss: 0.5349 - val_acc: 0.8103\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4073 - acc: 0.8537 - val_loss: 0.5338 - val_acc: 0.8130\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.4048 - acc: 0.8562 - val_loss: 0.5322 - val_acc: 0.8143\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4023 - acc: 0.8578 - val_loss: 0.5340 - val_acc: 0.8110\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4007 - acc: 0.8575 - val_loss: 0.5323 - val_acc: 0.8143\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3981 - acc: 0.8590 - val_loss: 0.5333 - val_acc: 0.8117\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3962 - acc: 0.8594 - val_loss: 0.5325 - val_acc: 0.8150\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3942 - acc: 0.8606 - val_loss: 0.5333 - val_acc: 0.8127\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3922 - acc: 0.8602 - val_loss: 0.5341 - val_acc: 0.8137\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3901 - acc: 0.8618 - val_loss: 0.5343 - val_acc: 0.8123\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3880 - acc: 0.8622 - val_loss: 0.5335 - val_acc: 0.8123\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3863 - acc: 0.8632 - val_loss: 0.5345 - val_acc: 0.8127\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3842 - acc: 0.8632 - val_loss: 0.5351 - val_acc: 0.8150\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3825 - acc: 0.8655 - val_loss: 0.5338 - val_acc: 0.8140\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3809 - acc: 0.8648 - val_loss: 0.5354 - val_acc: 0.8140\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3785 - acc: 0.8654 - val_loss: 0.5371 - val_acc: 0.8147\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3772 - acc: 0.8672 - val_loss: 0.5369 - val_acc: 0.8137\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3754 - acc: 0.8681 - val_loss: 0.5385 - val_acc: 0.8110\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3738 - acc: 0.8686 - val_loss: 0.5383 - val_acc: 0.8130\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3722 - acc: 0.8688 - val_loss: 0.5379 - val_acc: 0.8163\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3702 - acc: 0.8691 - val_loss: 0.5383 - val_acc: 0.8167\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3688 - acc: 0.8690 - val_loss: 0.5431 - val_acc: 0.8107\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3672 - acc: 0.8703 - val_loss: 0.5425 - val_acc: 0.8110\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3653 - acc: 0.8710 - val_loss: 0.5426 - val_acc: 0.8120\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3639 - acc: 0.8716 - val_loss: 0.5429 - val_acc: 0.8127\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3623 - acc: 0.8719 - val_loss: 0.5444 - val_acc: 0.8107\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3608 - acc: 0.8722 - val_loss: 0.5491 - val_acc: 0.8117\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3593 - acc: 0.8731 - val_loss: 0.5446 - val_acc: 0.8110\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3577 - acc: 0.8750 - val_loss: 0.5438 - val_acc: 0.8137\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3565 - acc: 0.8747 - val_loss: 0.5453 - val_acc: 0.8123\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3551 - acc: 0.8752 - val_loss: 0.5464 - val_acc: 0.8110\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3532 - acc: 0.8755 - val_loss: 0.5476 - val_acc: 0.8127\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3520 - acc: 0.8755 - val_loss: 0.5471 - val_acc: 0.8093\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3508 - acc: 0.8760 - val_loss: 0.5488 - val_acc: 0.8117\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3491 - acc: 0.8770 - val_loss: 0.5478 - val_acc: 0.8113\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3479 - acc: 0.8772 - val_loss: 0.5502 - val_acc: 0.8123\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3464 - acc: 0.8781 - val_loss: 0.5556 - val_acc: 0.8117\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3455 - acc: 0.8774 - val_loss: 0.5503 - val_acc: 0.8110\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3438 - acc: 0.8788 - val_loss: 0.5564 - val_acc: 0.8123\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3423 - acc: 0.8790 - val_loss: 0.5530 - val_acc: 0.8123\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3411 - acc: 0.8793 - val_loss: 0.5540 - val_acc: 0.8133\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3398 - acc: 0.8800 - val_loss: 0.5537 - val_acc: 0.8120\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3380 - acc: 0.8796 - val_loss: 0.5543 - val_acc: 0.8130\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3373 - acc: 0.8819 - val_loss: 0.5558 - val_acc: 0.8120\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3360 - acc: 0.8816 - val_loss: 0.5594 - val_acc: 0.8110\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3348 - acc: 0.8819 - val_loss: 0.5594 - val_acc: 0.8103\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3334 - acc: 0.8825 - val_loss: 0.5603 - val_acc: 0.8100\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3319 - acc: 0.8835 - val_loss: 0.5626 - val_acc: 0.8080\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3310 - acc: 0.8840 - val_loss: 0.5615 - val_acc: 0.8093\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3296 - acc: 0.8844 - val_loss: 0.5637 - val_acc: 0.8090\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3285 - acc: 0.8842 - val_loss: 0.5645 - val_acc: 0.8097\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3276 - acc: 0.8858 - val_loss: 0.5649 - val_acc: 0.8097\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3261 - acc: 0.8852 - val_loss: 0.5661 - val_acc: 0.8080\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3253 - acc: 0.8852 - val_loss: 0.5664 - val_acc: 0.8107\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3239 - acc: 0.8859 - val_loss: 0.5667 - val_acc: 0.8107\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3226 - acc: 0.8865 - val_loss: 0.5671 - val_acc: 0.8100\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3219 - acc: 0.8872 - val_loss: 0.5701 - val_acc: 0.8123\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3206 - acc: 0.8876 - val_loss: 0.5690 - val_acc: 0.8067\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3196 - acc: 0.8875 - val_loss: 0.5713 - val_acc: 0.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3181 - acc: 0.8887 - val_loss: 0.5707 - val_acc: 0.8097\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3172 - acc: 0.8894 - val_loss: 0.5745 - val_acc: 0.8087\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3164 - acc: 0.8892 - val_loss: 0.5745 - val_acc: 0.8087\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 37us/step\n",
      "4000/4000 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31161378081639607, 0.8917575757575757]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5738522683382035, 0.804]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
